{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# <font color = 'indianred'>**HW2_Part_B - 10 Points** </font>\n","- **You have to submit only one file for this part of the HW**\n","  >(1) ipynb (colab notebook) and<br>\n","\n","- **File should be named as follows**:\n",">FirstName_LastName_HW_2_part_B<br>"],"metadata":{"id":"MGjSyy3q9fJp"}},{"cell_type":"markdown","source":["**Objective**: Learn how to solve a Multiclass classification problem using neural network in PyTorch.\n","\n","**Task**:In this HW, you will implement a MultiClass classification using the same house pricing dataset previously used for a regression task. To adapt the dataset for multiclass classification, we have categorized the dataset's numeric response (target) variable into three classes: low, medium, and high, represented by 0, 1, and 2, respectively. The data has also been scaled to facilitate this analysis. The modified dataset is ready for you in the CSV file CaliforniaHousing_classification.csv\n","\n","For this class exsrcise you will again train two models.\n","\n",">Model 1: Train model without accounting for imbalance <br>\n",">Model 2: Train model accounting for imbalance\n","\n","**Plan**:\n","1. Set Environment: Load and understand the libraries needed.\n","2. Load Data: Load house price data for this lab and explore it.\n","3. Split Data: Learn how to split data into training, validation and testing\n","4. Create Datasets\n","5. Define functions we need to train and evaluate Models 1, and 2.\n","6. Model 1 : Train Model 1 (without accounting for Imbalance)\n","> 6.1. Define Hyperparameters <br>\n","> 6.2. Define Training Configurations <br>\n","> 6.3. Model Training and Evaluation <br>\n","\n","7. Model 2: Train Model 2 (accounting for Imbalance)\n","> 7.1. Define Hyperparameters <br>\n","> 7.2. Define Training Configurations <br>\n","> 7.3. Model Training and Evaluation <br>\n","\n","\n","**For the HW**,\n","- enter your code into the cells that contain the placeholder **# CODE.**\n","- Answer question in marksdown cell with **Your response here**\n","\n","**Note**: For the HW, we will focus only on training the model and we will neither do model evaluation nor inference."],"metadata":{"id":"QUPJ_8CVPdMH"}},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**1. Set Environment**"],"metadata":{"id":"ICwSEieuV4dO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL_n1VZ2sWLT"},"outputs":[],"source":["# sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import confusion_matrix\n","\n","# Data handling and visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","# file manipulation\n","from google.colab import drive\n","from pathlib import Path\n","\n","# Pytorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","source":["# Determine the storage location based on the execution environment\n","# If running on Google Colab, use Google Drive as storage\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive  # Import Google Drive mounting utility\n","    drive.mount('/content/drive')  # Mount Google Drive\n","\n","    # Set base folder path for storing data on Google Drive\n","    base_folder= Path('/content/drive/MyDrive/data')\n","    project_folder = Path('/content/drive/MyDrive/teaching_fall_2025/LLM_Fall_2025/')\n","# If running locally, specify a different path\n","else:\n","    # Set base folder path for storing data on local machine\n","    base_folder= Path('/home/harpreet/Insync/google_drive_shaannoor/data')\n","    project_folder = Path('/home/harpreet/Insync/google_drive_shaannoor/teaching_fall_2025/LLM_Fall_2025/')"],"metadata":{"id":"Kz1VDDrJYljL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805101333,"user_tz":360,"elapsed":15113,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"546f218a-f2dd-4c59-a711-94861593c2fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Students can set the data_folder and model_folder paths based on their base_folder setup.\n","data_folder = base_folder/'datasets'\n","model_folder = project_folder/'0_saved_models/HW2_Part_B'\n","model_folder.mkdir(exist_ok=True, parents = True)\n","data_folder.mkdir(exist_ok=True, parents = True)"],"metadata":{"id":"uDsUtdSQoprh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**2. Load Data**\n","\n"],"metadata":{"id":"XuloIPO9V8Zs"}},{"cell_type":"code","source":["df = pd.read_csv(data_folder/'CaliforniaHousing_classification.csv')"],"metadata":{"id":"Xa8_oLyLuSDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"dG7CJRequTpv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105257,"user_tz":360,"elapsed":16,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"1c909ab3-19f9-4564-eba3-925c601eec2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20640 entries, 0 to 20639\n","Data columns (total 9 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   MedInc              20640 non-null  float64\n"," 1   HouseAge            20640 non-null  float64\n"," 2   AveRooms            20640 non-null  float64\n"," 3   AveBedrms           20640 non-null  float64\n"," 4   Population          20640 non-null  float64\n"," 5   AveOccup            20640 non-null  float64\n"," 6   Latitude            20640 non-null  float64\n"," 7   Longitude           20640 non-null  float64\n"," 8   HouseValueCategory  20640 non-null  int64  \n","dtypes: float64(8), int64(1)\n","memory usage: 1.4 MB\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"DnkqhfzmyhVf","colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"status":"ok","timestamp":1739805105258,"user_tz":360,"elapsed":13,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"faf9a708-9523-43cf-817a-ae2223ea89f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n","0  2.344766  0.982143  0.628559  -0.153758   -0.974429 -0.049597  1.052548   \n","1  2.332238 -0.607019  0.327041  -0.263336    0.861439 -0.092512  1.043185   \n","2  1.782699  1.856182  1.155620  -0.049016   -0.820777 -0.025843  1.038503   \n","3  0.932968  1.856182  0.156966  -0.049833   -0.766028 -0.050329  1.038503   \n","4 -0.012881  1.856182  0.344711  -0.032906   -0.759847 -0.085616  1.038503   \n","\n","   Longitude  HouseValueCategory  \n","0  -1.327835                   2  \n","1  -1.322844                   2  \n","2  -1.332827                   2  \n","3  -1.337818                   2  \n","4  -1.337818                   2  "],"text/html":["\n","  <div id=\"df-c651ddd0-8cf4-4db0-af33-1cf97325ca17\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MedInc</th>\n","      <th>HouseAge</th>\n","      <th>AveRooms</th>\n","      <th>AveBedrms</th>\n","      <th>Population</th>\n","      <th>AveOccup</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","      <th>HouseValueCategory</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.344766</td>\n","      <td>0.982143</td>\n","      <td>0.628559</td>\n","      <td>-0.153758</td>\n","      <td>-0.974429</td>\n","      <td>-0.049597</td>\n","      <td>1.052548</td>\n","      <td>-1.327835</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.332238</td>\n","      <td>-0.607019</td>\n","      <td>0.327041</td>\n","      <td>-0.263336</td>\n","      <td>0.861439</td>\n","      <td>-0.092512</td>\n","      <td>1.043185</td>\n","      <td>-1.322844</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.782699</td>\n","      <td>1.856182</td>\n","      <td>1.155620</td>\n","      <td>-0.049016</td>\n","      <td>-0.820777</td>\n","      <td>-0.025843</td>\n","      <td>1.038503</td>\n","      <td>-1.332827</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.932968</td>\n","      <td>1.856182</td>\n","      <td>0.156966</td>\n","      <td>-0.049833</td>\n","      <td>-0.766028</td>\n","      <td>-0.050329</td>\n","      <td>1.038503</td>\n","      <td>-1.337818</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.012881</td>\n","      <td>1.856182</td>\n","      <td>0.344711</td>\n","      <td>-0.032906</td>\n","      <td>-0.759847</td>\n","      <td>-0.085616</td>\n","      <td>1.038503</td>\n","      <td>-1.337818</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c651ddd0-8cf4-4db0-af33-1cf97325ca17')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c651ddd0-8cf4-4db0-af33-1cf97325ca17 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c651ddd0-8cf4-4db0-af33-1cf97325ca17');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2174b555-aa52-4e3b-b393-1aafdc147ee4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2174b555-aa52-4e3b-b393-1aafdc147ee4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2174b555-aa52-4e3b-b393-1aafdc147ee4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 20640,\n  \"fields\": [\n    {\n      \"column\": \"MedInc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.000024225686497,\n        \"min\": -1.7742994673175232,\n        \"max\": 5.858285811780286,\n        \"num_unique_values\": 12928,\n        \"samples\": [\n          0.6095082700550821,\n          -0.961887768185128,\n          1.1854710024580022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HouseAge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.000024225686517,\n        \"min\": -2.1961804849268263,\n        \"max\": 1.8561815225324745,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5053941867127075,\n          -0.2891865990636258,\n          -1.719432013461026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveRooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864986,\n        \"min\": -1.852318597109508,\n        \"max\": 55.16323628125675,\n        \"num_unique_values\": 19392,\n        \"samples\": [\n          0.2757634016107566,\n          0.1955532067707373,\n          0.1469023679496574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveBedrms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864993,\n        \"min\": -1.6107677167688603,\n        \"max\": 69.5717132557033,\n        \"num_unique_values\": 14233,\n        \"samples\": [\n          -0.223720370421143,\n          0.0325480371882086,\n          -0.1199666925194045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864973,\n        \"min\": -1.2561225469018058,\n        \"max\": 30.250330218731506,\n        \"num_unique_values\": 3888,\n        \"samples\": [\n          2.422676809067486,\n          -0.6971499131215462,\n          1.7144681956523704\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveOccup\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864975,\n        \"min\": -0.2289999744351198,\n        \"max\": 119.41910318829312,\n        \"num_unique_values\": 18841,\n        \"samples\": [\n          -0.0362682990568809,\n          0.0470565518881564,\n          0.0218016053818307\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864895,\n        \"min\": -1.447567998357702,\n        \"max\": 2.958067621103192,\n        \"num_unique_values\": 862,\n        \"samples\": [\n          -0.9044715776802684,\n          -0.572059113300119,\n          1.2210954480745082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000242256864942,\n        \"min\": -2.385992341673388,\n        \"max\": 2.625280057018667,\n        \"num_unique_values\": 844,\n        \"samples\": [\n          0.4690353595734818,\n          -0.1448954223379974,\n          -0.8436784261396818\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HouseValueCategory\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["df['HouseValueCategory'].value_counts(normalize=True)*100"],"metadata":{"id":"fbLGoc9_lY0B","colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"status":"ok","timestamp":1739805105258,"user_tz":360,"elapsed":10,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"78b90c6e-c741-48cc-d00b-20c859351dda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HouseValueCategory\n","1    70.179264\n","2    16.191860\n","0    13.628876\n","Name: proportion, dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>proportion</th>\n","    </tr>\n","    <tr>\n","      <th>HouseValueCategory</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>70.179264</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16.191860</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>13.628876</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["The value counts for the 'HouseValueCategory' in the dataset reveal a significant imbalance among the different categories of house prices: low (0), medium (1), and high (2). Specifically, medium-priced houses constitute a majority with approximately 70.18% of the data, followed by high-priced houses at about 16.19%, and low-priced houses at 13.63%."],"metadata":{"id":"JdMAzZnblnH1"}},{"cell_type":"code","source":["# Drop the target column, HouseValueCategory, from inputs, X; y is the target column\n","X = df.drop(['HouseValueCategory'], axis=1).values\n","y = df['HouseValueCategory'].values"],"metadata":{"id":"lvq_vCcDWX-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape, y.shape"],"metadata":{"id":"-zbULm78YH9p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105396,"user_tz":360,"elapsed":22,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"65fb9ece-d2a2-4e04-f11e-49adec7717e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((20640, 8), (20640,))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["type(X), type(y)"],"metadata":{"id":"sfQppw6YXkrw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105396,"user_tz":360,"elapsed":19,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"ef3d9014-7ada-4649-dedd-356c65bd1730"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(numpy.ndarray, numpy.ndarray)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["X.shape, y.shape"],"metadata":{"id":"RVPUvdRIYeqk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105397,"user_tz":360,"elapsed":17,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e652c0b2-7b44-4c79-e659-395b6580935f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((20640, 8), (20640,))"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**3. Split Data**"],"metadata":{"id":"R7IKH_clWCm7"}},{"cell_type":"code","source":["# Splitting the data into training and temporary sets (60% - 40%)\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n","\n","# Splitting the temporary set into validation and test sets (50% - 50% of 40%)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n","\n","# Print the shapes of the splits\n","print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Validation set shape:\", X_val.shape, y_val.shape)\n","print(\"Test set shape:\", X_test.shape, y_test.shape)"],"metadata":{"id":"M3QH3qgewvia","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105397,"user_tz":360,"elapsed":15,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"87249cd4-388d-41a5-fdc1-5a9a82ead8ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape: (12384, 8) (12384,)\n","Validation set shape: (4128, 8) (4128,)\n","Test set shape: (4128, 8) (4128,)\n"]}]},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**4. Create Dataset**\n"],"metadata":{"id":"xlkN0DLdWI-s"}},{"cell_type":"code","source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, X, y):\n","\n","    # Storing feature data (texts)\n","    self.X = X\n","    # Storing the target labels\n","    self.y = y\n","\n","  def __len__(self):\n","    return len(self.X)\n","\n","  def __getitem__(self, idx):\n","\n","    # Retrieve the text and corresponding label from the dataset using the index\n","    inputs = self.X[idx]\n","    labels = self.y[idx]\n","\n","    inputs = torch.tensor(inputs, dtype=torch.float32)\n","    # For multi-class classification, we use  long instead of float32\n","    # This is because each label is an integer  (0,1,2,3,4,.....)\n","    labels = ## Code Here\n","\n","    # Packing them into a tuple before returning\n","    sample = (inputs, labels)\n","\n","    return sample"],"metadata":{"id":"4XCJV8FXpCJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create TensorDatasets for each set\n","train_dataset = CustomDataset(X_train, y_train)\n","val_dataset = CustomDataset(X_val, y_val)\n","test_dataset = CustomDataset(X_test, y_test)\n","train_dataset, val_dataset, test_dataset"],"metadata":{"id":"xRAWkIYtxzIF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739805105397,"user_tz":360,"elapsed":14,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"64aad760-2114-480b-d9e5-25c626f22b72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<__main__.CustomDataset at 0x7cb5509a8c10>,\n"," <__main__.CustomDataset at 0x7cb5509a8c50>,\n"," <__main__.CustomDataset at 0x7cb5509a8d90>)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"aqI_o6qwy6lb"},"source":["# <Font color = 'indianred'>**5. Functions to train & evaluate Models**\n","\n"]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*`step()` function*"],"metadata":{"id":"ndebVBmw_GMp"}},{"cell_type":"code","source":["def step(inputs, targets, model, device, loss_function=None, optimizer=None):\n","    \"\"\"\n","    Performs a forward and backward pass for a given batch of inputs and targets.\n","\n","    Parameters:\n","    - inputs (torch.Tensor): The input data for the model.\n","    - targets (torch.Tensor): The true labels for the input data.\n","    - model (torch.nn.Module): The neural network model.\n","    - device (torch.device): The computing device (CPU or GPU).\n","    - loss_function (torch.nn.Module, optional): The loss function to calculate loss.\n","    - optimizer (torch.optim.Optimizer, optional): The optimizer to update model parameters.\n","\n","    Returns:\n","    - loss (float): The computed loss value (only if loss_function is not None).\n","    - outputs (torch.Tensor): The predictions from the model.\n","    - correct (int): The number of correctly classified samples in the batch.\n","    \"\"\"\n","    # Move the model and data to the device\n","    model = model.to(device)\n","    inputs = inputs.to(device)\n","\n","    targets = targets.to(device)\n","\n","    # Step 1: Forward pass to get the model's predictions\n","    outputs = model(inputs)\n","\n","    # Step 2a: Compute the loss using the provided loss function\n","    if loss_function:\n","        loss = loss_function(outputs, targets)\n","\n","    # Step 2b: Calculate the number of correctly classified samples\n","    predicted = ## Code Here # outputs are logits , you need the predicted class\n","    correct = (predicted == targets).sum().item()\n","\n","    # Step 3 and 4: Perform backward pass and update model parameters if an optimizer is provided\n","    if optimizer:\n","        # Step : Zero Gradients - Clear previous gradient information to prevent accumulation\n","        # Code HERE\n","\n","        # Step : Calculate Gradients - Backpropagate the error to compute gradients for each parameter\n","        # Code HERE\n","\n","        # Step : Update Model Parameters - Adjust weights based on computed gradients\n","        # Code HERE\n","\n","    if loss_function:\n","      return loss, outputs, correct\n","    else:\n","      return None, outputs, correct"],"metadata":{"id":"GEM8sbJu8ADo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*`train_epoch()` function*"],"metadata":{"id":"bkT0zT_XYL4s"}},{"cell_type":"code","source":["def train_epoch(train_loader, model, device, loss_function, optimizer):\n","    \"\"\"\n","    Trains the model for one epoch using the provided data loader and updates the model parameters.\n","\n","    Parameters:\n","    - train_loader (torch.utils.data.DataLoader): DataLoader object for the training set.\n","    - model (torch.nn.Module): The neural network model to be trained.\n","    - device (torch.device): The computing device (CPU or GPU).\n","    - loss_function (torch.nn.Module): The loss function to calculate loss.\n","    - optimizer (torch.optim.Optimizer): The optimizer to update model parameters.\n","\n","    Returns:\n","    - train_loss (float): Average training loss for the epoch.\n","    - train_acc (float): Training accuracy for the epoch.\n","    \"\"\"\n","    # Set the model to training mode\n","    model.train()\n","\n","    # Initialize variables to track running training loss and correct predictions\n","    running_train_loss = 0.0\n","    running_train_correct = 0\n","\n","    # Iterate over all batches in the training data\n","    for inputs, targets in train_loader:\n","        # Perform a forward and backward pass, updating model parameters\n","        loss, _, correct = step(inputs, targets, model, device, loss_function, optimizer)\n","\n","        # Update running loss and correct predictions counter\n","        running_train_loss += loss.item()\n","        running_train_correct += correct\n","\n","    # Compute average loss and accuracy for the entire training set\n","    train_loss = running_train_loss / len(train_loader)\n","    train_acc = running_train_correct / len(train_loader.dataset)\n","\n","    return train_loss, train_acc\n"],"metadata":{"id":"J2BpCxsrAnqf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*`val_epoch()` function*"],"metadata":{"id":"_eMtYWiBYSvv"}},{"cell_type":"code","source":["def val_epoch(valid_loader, model, device, loss_function):\n","    \"\"\"\n","    Validates the model for one epoch using the provided data loader.\n","\n","    Parameters:\n","    - valid_loader (torch.utils.data.DataLoader): DataLoader object for the validation set.\n","    - model (torch.nn.Module): The neural network model to be validated.\n","    - device (torch.device): The computing device (CPU or GPU).\n","    - loss_function (torch.nn.Module): The loss function to calculate loss.\n","\n","    Returns:\n","    - val_loss (float): Average validation loss for the epoch.\n","    - val_acc (float): Validation accuracy for the epoch.\n","    \"\"\"\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    # Initialize variables to track running validation loss and correct predictions\n","    running_val_loss = 0.0\n","    running_val_correct = 0\n","\n","    # Disable gradient computation\n","    with torch.no_grad():\n","        # Iterate over all batches in the validation data\n","        for inputs, targets in valid_loader:\n","            # Perform a forward pass to get loss and number of correct predictions\n","            loss, _, correct = step(inputs, targets, model, device, loss_function, optimizer=None)\n","\n","            # Update running loss and correct predictions counter\n","            running_val_loss += loss.item()\n","            running_val_correct += correct\n","\n","    # Compute average loss and accuracy for the entire validation set\n","    val_loss = running_val_loss / len(valid_loader)\n","    val_acc = running_val_correct / len(valid_loader.dataset)\n","\n","    return val_loss, val_acc\n"],"metadata":{"id":"uCGiGJtkAxQe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*`train()` function*"],"metadata":{"id":"rp-VIoEBYWDg"}},{"cell_type":"code","source":["def train(train_loader, valid_loader, model, optimizer, loss_function, epochs, device):\n","    \"\"\"\n","    Trains and validates the model, and returns history of train and validation metrics.\n","\n","    Parameters:\n","    - train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n","    - valid_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n","    - model (torch.nn.Module): Neural network model to train.\n","    - optimizer (torch.optim.Optimizer): Optimizer algorithm.\n","    - loss_function (torch.nn.Module): Loss function to to calculate loss.\n","    - epochs (int): Number of epochs to train the model.\n","    - device (torch.device): The computing device (CPU or GPU).\n","\n","    Returns:\n","    - train_loss_history (list): History of training loss for each epoch.\n","    - train_acc_history (list): History of training accuracy for each epoch.\n","    - valid_loss_history (list): History of validation loss for each epoch.\n","    - valid_acc_history (list): History of validation accuracy for each epoch.\n","    \"\"\"\n","\n","    # Initialize lists to store metrics for each epoch\n","    train_loss_history = []\n","    valid_loss_history = []\n","    train_acc_history = []\n","    valid_acc_history = []\n","\n","    # Loop over the number of specified epochs\n","    for epoch in range(epochs):\n","        # Train model on training data and capture metrics\n","        train_loss, train_acc = train_epoch(train_loader, model, device, loss_function, optimizer)\n","\n","        # Validate model on validation data and capture metrics\n","        valid_loss, valid_acc = val_epoch(valid_loader, model, device, loss_function)\n","\n","        # Store metrics for this epoch\n","        train_loss_history.append(train_loss)\n","        train_acc_history.append(train_acc)\n","        valid_loss_history.append(valid_loss)\n","        valid_acc_history.append(valid_acc)\n","\n","        # Output epoch-level summary\n","        print(f\"Epoch {epoch+1}/{epochs}\")\n","        print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc*100:.2f}%\")\n","        print(f\"Valid Loss: {valid_loss:.4f} | Valid Accuracy: {valid_acc*100:.2f}%\")\n","        print()\n","\n","    return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history\n"],"metadata":{"id":"kBKa4xPxA9-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**6. Train Model 1 (without accounting for imbalance)**"],"metadata":{"id":"Oq_ynnOSUK8F"}},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*6.1 Hyperparameters*"],"metadata":{"id":"CYTZYGNsUK8G"}},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 64\n","learning_rate = 0.1\n","number_inputs = 8\n","number_outputs = # CODE HERE\n","number_hidden_1 = 100\n","number_hidden_2 = 100\n","epochs = 10"],"metadata":{"id":"r1ulxYcOUK8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*6.2 Training Configurations*"],"metadata":{"id":"fADJ5JJOUK8G"}},{"cell_type":"code","source":["# Fixing the seed value for reproducibility across runs\n","SEED = 2345\n","random.seed(SEED)                     # Set seed for Python's 'random' module\n","np.random.seed(SEED)                  # Set seed for NumPy's random number generation\n","torch.manual_seed(SEED)               # Set seed for PyTorch's CPU operations\n","torch.cuda.manual_seed(SEED)          # Set seed for PyTorch's CUDA (GPU) operations\n","torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in CuDNN\n","\n","# Create data loaders\n","# Hint : Keep in mind which datasets you wnat to shuffle and which you do not want to shuffle\n","train_loader = ## Code Here\n","val_loader = ## Code Here\n","test_loader = ## Code Here\n","\n","# Define the Cross Entropy  Loss function\n","loss_function = # CODE HERE\n","\n","# model\n","hidden_1 = # CODE HERE\n","hidden_2 = # CODE HERE\n","output_layer = # CODE HERE\n","model_1 = # CODE HERE (Hint: Use nn.Sequential)\n","\n","\n","# Define the optimizer - use SGD\n","optimizer = # CODE HERE\n","\n","# Define the device for model training (use CUDA if available, else CPU)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"NnlblHMnUK8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*6.3 Moddel Training*"],"metadata":{"id":"GOo7Fc1ApKNV"}},{"cell_type":"code","source":["# Call the train function to train the model\n","train_losses, train_acc, valid_losses, valid_acc  = train(\n","    train_loader, val_loader, model_1, optimizer, loss_function, epochs, device)"],"metadata":{"id":"w-5Wg1edUK8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Plot losses**"],"metadata":{"id":"u2xlinMzsz3u"}},{"cell_type":"code","source":["def plot_history(train_losses, train_metrics, val_losses=None, val_metrics=None):\n","    \"\"\"\n","    Plot training and validation loss and metrics over epochs.\n","\n","    Args:\n","        train_losses (list): List of training losses for each epoch.\n","        train_metrics (list): List of training metrics (e.g., accuracy) for each epoch.\n","        val_losses (list, optional): List of validation losses for each epoch.\n","        val_metrics (list, optional): List of validation metrics for each epoch.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Determine the number of epochs based on the length of train_losses\n","    epochs = range(1, len(train_losses) + 1)\n","\n","    # Plotting training and validation losses\n","    plt.figure()\n","    plt.plot(epochs, train_losses, label=\"Train\")  # Plot training losses\n","    if val_losses:  # Check if validation losses are provided\n","        plt.plot(epochs, val_losses, label=\"Validation\")  # Plot validation losses\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.show()\n","\n","    # Plotting training and validation metrics\n","    if train_metrics[0] is not None:  # Check if training metrics are available\n","        plt.figure()\n","        plt.plot(epochs, train_metrics, label=\"Train\")  # Plot training metrics\n","        if val_metrics:  # Check if validation metrics are provided\n","            plt.plot(epochs, val_metrics, label=\"Validation\")  # Plot validation metrics\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Metric\")\n","        plt.legend()\n","        plt.show()\n"],"metadata":{"id":"P2XorLQaWSIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(train_losses, train_acc, valid_losses, valid_acc)"],"metadata":{"id":"56hxfHZJUK8H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model checkpointing**\n","\n"],"metadata":{"id":"nkqhfdRRtTDW"}},{"cell_type":"code","source":["# file name for model to save\n","file_name_1 = model_folder/'two_layer.pt'"],"metadata":{"id":"-Bz_RghE3njL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(## Code Here, file_name_1)"],"metadata":{"id":"paWTQrIf3njL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate model on validation dataset**\n"],"metadata":{"id":"SIS-_lJetAeS"}},{"cell_type":"code","source":["def get_acc_pred(data_loader, model, device):\n","    \"\"\"\n","    Function to get predictions and accuracy for a given data using a trained model\n","    Input: data iterator, model, device\n","    Output: predictions and accuracy for the given dataset\n","    \"\"\"\n","    model = model.to(device)\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Create empty tensors to store predictions and actual labels\n","    predictions = torch.Tensor().to(device)\n","    y = torch.Tensor().to(device)\n","    running_correct = 0\n","    # Iterate over batches from data iterator\n","    with torch.no_grad():\n","        for inputs, targets in data_loader:\n","            # Process the batch to get the loss, outputs, and correct predictions\n","            _, outputs, correct = step(inputs, targets, model,\n","                              device, loss_function=None, optimizer=None)\n","\n","            # Choose the label with maximum probability\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            # Add the predicted labels and actual labels to their respective tensors\n","            predictions = torch.cat((predictions, predicted))\n","            y = torch.cat((y, targets.to(device)))\n","            running_correct += correct\n","\n","    # Calculate accuracy by comparing the predicted and actual labels\n","    accuracy = running_correct / len(data_loader.dataset)\n","\n","    # Return tuple containing predictions and accuracy\n","    return predictions, y, accuracy"],"metadata":{"id":"8B2q3y4rEiQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_valid, labels_valid, acc_valid = get_acc_pred(val_loader, model_1, device)"],"metadata":{"id":"Jeue29qhEvVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc_valid"],"metadata":{"id":"lgIDEJ2YE4hc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(valid_labels, valid_preds, class_labels):\n","    \"\"\"\n","    Plots a confusion matrix.\n","\n","    Args:\n","        valid_labels (array-like): True labels of the validation data.\n","        valid_preds (array-like): Predicted labels of the validation data.\n","        class_labels (list): List of class names for the labels.\n","    \"\"\"\n","    # Compute the confusion matrix\n","    cm = confusion_matrix(valid_labels, valid_preds, normalize='true')\n","\n","    # Plot the confusion matrix using Seaborn\n","    # 'ax' represents the axes of the plot. Seaborn's heatmap is used for visualizing the confusion matrix\n","    # 'annot=True' displays the values in the cells, and 'fmt' specifies the string formatting for these values\n","    plt.figure(figsize=(5, 4))\n","    ax = sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Reds\", xticklabels=class_labels, yticklabels=class_labels)\n","\n","    # Rotating the tick labels for better readability\n","    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n","    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n","\n","    # Adding labels and title to the plot\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.title('Confusion Matrix')\n","\n","    # Display the plot\n","    plt.show()\n"],"metadata":{"id":"dcOoPZOqGAjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_confusion_matrix( labels_valid.cpu().numpy(), predictions_valid.cpu().numpy(), ['low', 'medium', 'high'])"],"metadata":{"id":"eSnfT6vgGSVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What do you conclude from the confusion matrix?**\n","\n","(Hint: refer to the section2 , where we calculatee the count_values for the response)\n","\n","**Your Response here:**\n","\n"],"metadata":{"id":"xUqzuUH1h22i"}},{"cell_type":"markdown","source":["# <Font color = 'indianred'>**7. Train Model 2 (accounting for imbalance)**\n","\n"],"metadata":{"id":"YwM1dzSUVOb9"}},{"cell_type":"code","source":["# Get unique values and their counts\n","unique, counts = np.unique(y_train, return_counts=True)\n","\n","# Create a dictionary for value counts\n","value_counts = dict(zip(unique, counts))\n","\n","print(value_counts)"],"metadata":{"id":"7PMef_hwcAi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Class counts: Low, Medium, High\n","class_counts = list(value_counts.values())\n","\n","# Total number of samples\n","total = sum(class_counts)\n","\n","# Calculate weights for each class\n","weights = torch.tensor([total / c for c in class_counts], dtype=torch.float32)\n","weights = weights.to('cuda')\n","weights"],"metadata":{"id":"MLufxgaOlPVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*7.1 Hyperparameters*"],"metadata":{"id":"Goa-0ER7VOb9"}},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 64\n","learning_rate = 0.1\n","number_inputs = 8\n","number_outputs =  # CODE HERE\n","number_hidden_1 = 100\n","number_hidden_2 = 100\n","epochs = 10"],"metadata":{"id":"QBa9LZZSVOb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*7.2 Training Configurations*"],"metadata":{"id":"STNB-Mh7VOb-"}},{"cell_type":"code","source":["# Fixing the seed value for reproducibility across runs\n","SEED = 2345\n","random.seed(SEED)                     # Set seed for Python's 'random' module\n","np.random.seed(SEED)                  # Set seed for NumPy's random number generation\n","torch.manual_seed(SEED)               # Set seed for PyTorch's CPU operations\n","torch.cuda.manual_seed(SEED)          # Set seed for PyTorch's CUDA (GPU) operations\n","torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in CuDNN\n","\n","# Create data loaders\n","# Hint : Keep in mind which datasets you wnat to shuffle and which you do not want to shuffle\n","train_loader = ## Code Here\n","val_loader = ## Code Here\n","test_loader = ## Code Here\n","\n","# Define the Cross Entropy  Loss function\n","# Hint: For Model 2, remember that we are accounting for class imbalance.\n","# The torch.nn module has a loss function that is suitable for multiclass classification and allows you to pass class weights.\n","# You have already calculated these weights in the previous steps\n","loss_function = # CODE HERE\n","\n","# model\n","hidden_1 = # CODE HERE\n","hidden_2 = # CODE HERE\n","output_layer = # CODE HERE\n","model_1 = # CODE HERE\n","\n","\n","# Define the optimizer - use SGD\n","optimizer = # CODE HERE\n","\n","# Define the device for model training (use CUDA if available, else CPU)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"Y9knPj45VOb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <Font color = 'indianred'>*7.3 Model Training*"],"metadata":{"id":"DJI5ejrmpQBY"}},{"cell_type":"code","source":["# Call the train function to train the model\n","train_losses, train_acc, valid_losses, valid_acc  = train(\n","    train_loader, val_loader, model_2, optimizer, loss_function, epochs, device)"],"metadata":{"id":"yqt5W6QMVOb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Plot losses**"],"metadata":{"id":"jd99rLJ_svju"}},{"cell_type":"code","source":["plot_history(train_losses, train_acc, valid_losses, valid_acc)"],"metadata":{"id":"Wa0YFmjxVOb_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model checkpointing**"],"metadata":{"id":"CcZIN2Kz5T1T"}},{"cell_type":"code","source":["# file name for model to save\n","file_name_2 = model_folder/'two_layer_cost.pt'"],"metadata":{"id":"Cw2g_vLW5T1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model_2.state_dict(), file_name_2 )"],"metadata":{"id":"ENr0ZdrR5T1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate Model on validation set**"],"metadata":{"id":"oUB1buhns4BD"}},{"cell_type":"code","source":["predictions_valid, labels_valid, acc_valid = get_acc_pred(val_loader, model_2, device)"],"metadata":{"id":"-KIeIiaBsoeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc_valid"],"metadata":{"id":"fzdoIkltsqQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_confusion_matrix( labels_valid.cpu().numpy(), predictions_valid.cpu().numpy(), ['low', 'medium', 'high'])"],"metadata":{"id":"b06foDLasrqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What conclusion you draw from the confusion matrix?**\n","\n","**Your response here**\n","\n","\n"],"metadata":{"id":"q0oopJGcjsYC"}}]}