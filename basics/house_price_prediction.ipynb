{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24f77da",
   "metadata": {},
   "source": [
    "# house price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb404d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bbb6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('datasets/houseprice.csv',usecols=['SalePrice','MSSubClass','MSZoning','LotFrontage','LotArea','Street','YearBuilt','LotShape','1stFlrSF','2ndFlrSF']).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727f63ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass     0\n",
       "MSZoning       0\n",
       "LotFrontage    0\n",
       "LotArea        0\n",
       "Street         0\n",
       "LotShape       0\n",
       "YearBuilt      0\n",
       "1stFlrSF       0\n",
       "2ndFlrSF       0\n",
       "SalePrice      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2c6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   object \n",
      " 5   LotShape     1201 non-null   object \n",
      " 6   YearBuilt    1201 non-null   int64  \n",
      " 7   1stFlrSF     1201 non-null   int64  \n",
      " 8   2ndFlrSF     1201 non-null   int64  \n",
      " 9   SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371dcaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name MSSubClass are unique values are 15\n",
      "column name MSZoning are unique values are 5\n",
      "column name LotFrontage are unique values are 110\n",
      "column name LotArea are unique values are 869\n",
      "column name Street are unique values are 2\n",
      "column name LotShape are unique values are 4\n",
      "column name YearBuilt are unique values are 112\n",
      "column name 1stFlrSF are unique values are 678\n",
      "column name 2ndFlrSF are unique values are 368\n",
      "column name SalePrice are unique values are 597\n"
     ]
    }
   ],
   "source": [
    "# choose categorical features \n",
    "for i in df.columns:\n",
    "    print(f'column name {i} are unique values are {len(df[i].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5248976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4857fa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>today year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  1stFlrSF  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       856   \n",
       "1          20       RL         80.0     9600   Pave      Reg      1262   \n",
       "2          60       RL         68.0    11250   Pave      IR1       920   \n",
       "3          70       RL         60.0     9550   Pave      IR1       961   \n",
       "4          60       RL         84.0    14260   Pave      IR1      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  today year  \n",
       "0       854     208500          22  \n",
       "1         0     181500          49  \n",
       "2       866     223500          24  \n",
       "3       756     140000         110  \n",
       "4      1053     250000          25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['today year']=datetime.datetime.now().year-df['YearBuilt']\n",
    "df.drop('YearBuilt',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf98275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between categorical and contineous features\n",
    "cat_features=['MSSubClass','MSZoning','Street','LotShape']\n",
    "out_feature='SalePrice' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d93e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  20,  70,  50, 190,  45,  90, 120,  30,  80, 160,  75, 180,\n",
       "        40,  85], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MSSubClass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176e0c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders={}\n",
    "lbl_encoders['MSSubClass']=LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b8a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fucction for label encoding \n",
    "for feature in cat_features:\n",
    "    lbl_encoders[feature]=LabelEncoder()\n",
    "    df[feature]=lbl_encoders[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8352f5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>today year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0           5         3         65.0     8450       1         3       856   \n",
       "1           0         3         80.0     9600       1         3      1262   \n",
       "2           5         3         68.0    11250       1         0       920   \n",
       "3           6         3         60.0     9550       1         0       961   \n",
       "4           5         3         84.0    14260       1         0      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  today year  \n",
       "0       854     208500          22  \n",
       "1         0     181500          49  \n",
       "2       866     223500          24  \n",
       "3       756     140000         110  \n",
       "4      1053     250000          25  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b528f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stackinh and coverting into tensors \n",
    "cat_features=np.stack([df['MSSubClass'],df['MSZoning'],df['Street'],df['LotShape']],1)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be8d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy to tensors \n",
    "import torch\n",
    "cat_features=torch.tensor(cat_features,dtype=torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b11b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'today year']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continous variables \n",
    "cont_features=[]\n",
    "for i in df.columns:\n",
    "    if i in ['MSSubClass','MSZoning','Street','LotShape','SalePrice']:\n",
    "        pass \n",
    "    else:\n",
    "        cont_features.append(i)\n",
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f5bc899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    22.],\n",
       "        [   80.,  9600.,  1262.,     0.,    49.],\n",
       "        [   68., 11250.,   920.,   866.,    24.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    84.],\n",
       "        [   68.,  9717.,  1078.,     0.,    75.],\n",
       "        [   75.,  9937.,  1256.,     0.,    60.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacking continuous varaibles to a tensor \n",
    "cont_values=np.stack([df[i].values for i in cont_features],axis=1)\n",
    "cont_values=torch.tensor(cont_values,dtype=torch.float)\n",
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff43ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependent features \n",
    "y=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffdc6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 4]), torch.Size([1201, 5]), torch.Size([1201, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.shape,cont_values.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37292b2",
   "metadata": {},
   "source": [
    "# embeddings size for categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cba4954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings layers to decide output based on input \n",
    "cat_dim=[len(df[col].unique()) for col in ['MSSubClass','MSZoning','Street','LotShape']]\n",
    "cat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0135abf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### thunb rule what dimension output shoud be set based on the input dimension \n",
    "embedding_dim=[(x,min(50,(x+1)//2)) for x in cat_dim]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0893db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layers to pytorch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef29fd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e09b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1450,  1.9652, -0.8435,  ..., -0.8175,  1.9491,  0.4325],\n",
       "         [-2.8154,  0.7935, -0.3415,  ..., -1.3223,  0.6610, -1.1573],\n",
       "         [-0.1450,  1.9652, -0.8435,  ..., -0.8175,  1.9491,  0.4325],\n",
       "         ...,\n",
       "         [-0.5147, -1.2756, -0.7045,  ..., -1.3899,  0.8247,  0.5438],\n",
       "         [-2.8154,  0.7935, -0.3415,  ..., -1.3223,  0.6610, -1.1573],\n",
       "         [-2.8154,  0.7935, -0.3415,  ..., -1.3223,  0.6610, -1.1573]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.4949,  1.3916, -0.7723],\n",
       "         [-0.4949,  1.3916, -0.7723],\n",
       "         [-0.4949,  1.3916, -0.7723],\n",
       "         ...,\n",
       "         [-0.4949,  1.3916, -0.7723],\n",
       "         [-0.4949,  1.3916, -0.7723],\n",
       "         [-0.4949,  1.3916, -0.7723]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-1.1665],\n",
       "         [-1.1665],\n",
       "         [-1.1665],\n",
       "         ...,\n",
       "         [-1.1665],\n",
       "         [-1.1665],\n",
       "         [-1.1665]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 1.2755, -1.1338],\n",
       "         [ 1.2755, -1.1338],\n",
       "         [ 0.1123,  0.1324],\n",
       "         ...,\n",
       "         [ 1.2755, -1.1338],\n",
       "         [ 1.2755, -1.1338],\n",
       "         [ 1.2755, -1.1338]], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val=[]\n",
    "for i,e, in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_features[:,i]))\n",
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865f1e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1450,  1.9652, -0.8435,  ..., -1.1665,  1.2755, -1.1338],\n",
       "        [-2.8154,  0.7935, -0.3415,  ..., -1.1665,  1.2755, -1.1338],\n",
       "        [-0.1450,  1.9652, -0.8435,  ..., -1.1665,  0.1123,  0.1324],\n",
       "        ...,\n",
       "        [-0.5147, -1.2756, -0.7045,  ..., -1.1665,  1.2755, -1.1338],\n",
       "        [-2.8154,  0.7935, -0.3415,  ..., -1.1665,  1.2755, -1.1338],\n",
       "        [-2.8154,  0.7935, -0.3415,  ..., -1.1665,  1.2755, -1.1338]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.cat(embedding_val,1)\n",
    "z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1c273f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -1.4058,  ..., -1.9442,  2.1259, -0.0000],\n",
       "        [-0.0000,  1.3226, -0.5692,  ..., -1.9442,  0.0000, -1.8897],\n",
       "        [-0.2417,  3.2753, -1.4058,  ..., -0.0000,  0.1872,  0.2207],\n",
       "        ...,\n",
       "        [-0.0000, -0.0000, -0.0000,  ..., -1.9442,  0.0000, -0.0000],\n",
       "        [-4.6924,  0.0000, -0.5692,  ..., -0.0000,  2.1259, -1.8897],\n",
       "        [-4.6924,  1.3226, -0.0000,  ..., -0.0000,  0.0000, -1.8897]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement dropout \n",
    "dropout=nn.Dropout(0.4)\n",
    "final_embed=dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba3b1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a neural network \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_dim,n_cont,out_sz,layers,p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop=nn.Dropout(p)\n",
    "        self.bn_cont=nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layerlist=[]\n",
    "        n_emb=sum((out for inp,out in embedding_dim))\n",
    "        n_in=n_emb+n_cont \n",
    "\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in=i \n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        self.layers=nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self,x_cat,x_cont):\n",
    "        embeddings=[]\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x=torch.cat(embeddings,1)\n",
    "        x=self.emb_drop(x)\n",
    "        x_cont=self.bn_cont(x_cont)\n",
    "        x=torch.cat([x,x_cont],1)\n",
    "        x=self.layers(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97403977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fdef704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "model=FeedForwardNN(embedding_dim,len(cont_features),1,[100,50],p=0.4)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b40337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5f900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "607a43e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   65.,  8450.,   856.,   854.,    22.],\n",
       "         [   80.,  9600.,  1262.,     0.,    49.],\n",
       "         [   68., 11250.,   920.,   866.,    24.],\n",
       "         ...,\n",
       "         [   66.,  9042.,  1188.,  1152.,    84.],\n",
       "         [   68.,  9717.,  1078.,     0.,    75.],\n",
       "         [   75.,  9937.,  1256.,     0.,    60.]]),\n",
       " torch.Size([1201, 5]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values, cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "127092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "batch_size=1200\n",
    "test_size=int(batch_size*0.15)\n",
    "train_categorical=cat_features[:batch_size-test_size].cuda()\n",
    "test_categorical=cat_features[batch_size-test_size:batch_size].cuda()\n",
    "train_cont=cont_values[:batch_size-test_size].cuda()\n",
    "test_cont=cont_values[batch_size-test_size:batch_size].cuda()\n",
    "ytrain=y[:batch_size-test_size].cuda()\n",
    "y_test=y[batch_size-test_size:batch_size].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9330908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(ytrain),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8b5be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 and loss 200496.40625\n",
      "Epoch number: 11 and loss 200493.109375\n",
      "Epoch number: 21 and loss 200488.453125\n",
      "Epoch number: 31 and loss 200481.8125\n",
      "Epoch number: 41 and loss 200472.265625\n",
      "Epoch number: 51 and loss 200460.0\n",
      "Epoch number: 61 and loss 200444.5625\n",
      "Epoch number: 71 and loss 200426.84375\n",
      "Epoch number: 81 and loss 200406.875\n",
      "Epoch number: 91 and loss 200380.71875\n",
      "Epoch number: 101 and loss 200351.75\n",
      "Epoch number: 111 and loss 200318.6875\n",
      "Epoch number: 121 and loss 200284.046875\n",
      "Epoch number: 131 and loss 200243.140625\n",
      "Epoch number: 141 and loss 200205.5\n",
      "Epoch number: 151 and loss 200160.90625\n",
      "Epoch number: 161 and loss 200102.84375\n",
      "Epoch number: 171 and loss 200060.390625\n",
      "Epoch number: 181 and loss 199999.171875\n",
      "Epoch number: 191 and loss 199934.875\n",
      "Epoch number: 201 and loss 199874.5625\n",
      "Epoch number: 211 and loss 199808.15625\n",
      "Epoch number: 221 and loss 199730.40625\n",
      "Epoch number: 231 and loss 199659.53125\n",
      "Epoch number: 241 and loss 199584.1875\n",
      "Epoch number: 251 and loss 199491.015625\n",
      "Epoch number: 261 and loss 199414.546875\n",
      "Epoch number: 271 and loss 199308.125\n",
      "Epoch number: 281 and loss 199224.40625\n",
      "Epoch number: 291 and loss 199141.984375\n",
      "Epoch number: 301 and loss 199012.734375\n",
      "Epoch number: 311 and loss 198936.703125\n",
      "Epoch number: 321 and loss 198796.203125\n",
      "Epoch number: 331 and loss 198721.734375\n",
      "Epoch number: 341 and loss 198607.5625\n",
      "Epoch number: 351 and loss 198472.125\n",
      "Epoch number: 361 and loss 198352.890625\n",
      "Epoch number: 371 and loss 198254.578125\n",
      "Epoch number: 381 and loss 198075.671875\n",
      "Epoch number: 391 and loss 197972.140625\n",
      "Epoch number: 401 and loss 197838.4375\n",
      "Epoch number: 411 and loss 197699.28125\n",
      "Epoch number: 421 and loss 197576.0\n",
      "Epoch number: 431 and loss 197437.0625\n",
      "Epoch number: 441 and loss 197266.875\n",
      "Epoch number: 451 and loss 197102.9375\n",
      "Epoch number: 461 and loss 196976.125\n",
      "Epoch number: 471 and loss 196762.328125\n",
      "Epoch number: 481 and loss 196669.265625\n",
      "Epoch number: 491 and loss 196504.734375\n",
      "Epoch number: 501 and loss 196324.0\n",
      "Epoch number: 511 and loss 196162.46875\n",
      "Epoch number: 521 and loss 196010.65625\n",
      "Epoch number: 531 and loss 195736.890625\n",
      "Epoch number: 541 and loss 195567.25\n",
      "Epoch number: 551 and loss 195449.5625\n",
      "Epoch number: 561 and loss 195276.1875\n",
      "Epoch number: 571 and loss 195122.65625\n",
      "Epoch number: 581 and loss 194829.015625\n",
      "Epoch number: 591 and loss 194673.296875\n",
      "Epoch number: 601 and loss 194525.484375\n",
      "Epoch number: 611 and loss 194322.734375\n",
      "Epoch number: 621 and loss 194070.1875\n",
      "Epoch number: 631 and loss 193911.78125\n",
      "Epoch number: 641 and loss 193697.203125\n",
      "Epoch number: 651 and loss 193511.484375\n",
      "Epoch number: 661 and loss 193321.53125\n",
      "Epoch number: 671 and loss 193106.28125\n",
      "Epoch number: 681 and loss 192818.703125\n",
      "Epoch number: 691 and loss 192521.8125\n",
      "Epoch number: 701 and loss 192530.65625\n",
      "Epoch number: 711 and loss 192286.828125\n",
      "Epoch number: 721 and loss 191930.453125\n",
      "Epoch number: 731 and loss 191657.78125\n",
      "Epoch number: 741 and loss 191425.265625\n",
      "Epoch number: 751 and loss 191115.78125\n",
      "Epoch number: 761 and loss 190986.984375\n",
      "Epoch number: 771 and loss 190831.984375\n",
      "Epoch number: 781 and loss 190556.390625\n",
      "Epoch number: 791 and loss 190180.96875\n",
      "Epoch number: 801 and loss 190146.921875\n",
      "Epoch number: 811 and loss 189747.203125\n",
      "Epoch number: 821 and loss 189544.0625\n",
      "Epoch number: 831 and loss 189316.65625\n",
      "Epoch number: 841 and loss 188971.125\n",
      "Epoch number: 851 and loss 188646.53125\n",
      "Epoch number: 861 and loss 188521.640625\n",
      "Epoch number: 871 and loss 188103.703125\n",
      "Epoch number: 881 and loss 187909.25\n",
      "Epoch number: 891 and loss 187593.234375\n",
      "Epoch number: 901 and loss 187324.40625\n",
      "Epoch number: 911 and loss 186994.65625\n",
      "Epoch number: 921 and loss 186829.65625\n",
      "Epoch number: 931 and loss 186528.796875\n",
      "Epoch number: 941 and loss 186188.953125\n",
      "Epoch number: 951 and loss 185763.375\n",
      "Epoch number: 961 and loss 185527.8125\n",
      "Epoch number: 971 and loss 185218.90625\n",
      "Epoch number: 981 and loss 184924.734375\n",
      "Epoch number: 991 and loss 184686.453125\n",
      "Epoch number: 1001 and loss 184302.8125\n",
      "Epoch number: 1011 and loss 184006.84375\n",
      "Epoch number: 1021 and loss 183741.546875\n",
      "Epoch number: 1031 and loss 183275.5\n",
      "Epoch number: 1041 and loss 183200.671875\n",
      "Epoch number: 1051 and loss 182937.09375\n",
      "Epoch number: 1061 and loss 182250.921875\n",
      "Epoch number: 1071 and loss 182161.484375\n",
      "Epoch number: 1081 and loss 181974.578125\n",
      "Epoch number: 1091 and loss 181363.078125\n",
      "Epoch number: 1101 and loss 181182.671875\n",
      "Epoch number: 1111 and loss 181148.515625\n",
      "Epoch number: 1121 and loss 180400.15625\n",
      "Epoch number: 1131 and loss 180146.140625\n",
      "Epoch number: 1141 and loss 179926.25\n",
      "Epoch number: 1151 and loss 179232.375\n",
      "Epoch number: 1161 and loss 179117.078125\n",
      "Epoch number: 1171 and loss 178925.34375\n",
      "Epoch number: 1181 and loss 178617.0625\n",
      "Epoch number: 1191 and loss 178062.625\n",
      "Epoch number: 1201 and loss 177804.078125\n",
      "Epoch number: 1211 and loss 177428.484375\n",
      "Epoch number: 1221 and loss 177403.1875\n",
      "Epoch number: 1231 and loss 176640.515625\n",
      "Epoch number: 1241 and loss 176215.796875\n",
      "Epoch number: 1251 and loss 175860.34375\n",
      "Epoch number: 1261 and loss 175829.765625\n",
      "Epoch number: 1271 and loss 175286.25\n",
      "Epoch number: 1281 and loss 174914.875\n",
      "Epoch number: 1291 and loss 174638.265625\n",
      "Epoch number: 1301 and loss 174171.546875\n",
      "Epoch number: 1311 and loss 173638.15625\n",
      "Epoch number: 1321 and loss 173367.34375\n",
      "Epoch number: 1331 and loss 173189.375\n",
      "Epoch number: 1341 and loss 172688.0625\n",
      "Epoch number: 1351 and loss 172493.5625\n",
      "Epoch number: 1361 and loss 171753.265625\n",
      "Epoch number: 1371 and loss 171770.453125\n",
      "Epoch number: 1381 and loss 170898.640625\n",
      "Epoch number: 1391 and loss 170261.59375\n",
      "Epoch number: 1401 and loss 170041.25\n",
      "Epoch number: 1411 and loss 170030.28125\n",
      "Epoch number: 1421 and loss 169583.84375\n",
      "Epoch number: 1431 and loss 169577.484375\n",
      "Epoch number: 1441 and loss 168634.234375\n",
      "Epoch number: 1451 and loss 168633.921875\n",
      "Epoch number: 1461 and loss 167501.328125\n",
      "Epoch number: 1471 and loss 167054.453125\n",
      "Epoch number: 1481 and loss 167301.84375\n",
      "Epoch number: 1491 and loss 166856.953125\n",
      "Epoch number: 1501 and loss 166223.734375\n",
      "Epoch number: 1511 and loss 165849.578125\n",
      "Epoch number: 1521 and loss 165897.90625\n",
      "Epoch number: 1531 and loss 164621.140625\n",
      "Epoch number: 1541 and loss 164828.390625\n",
      "Epoch number: 1551 and loss 164113.5625\n",
      "Epoch number: 1561 and loss 164261.59375\n",
      "Epoch number: 1571 and loss 163236.078125\n",
      "Epoch number: 1581 and loss 162547.921875\n",
      "Epoch number: 1591 and loss 162474.15625\n",
      "Epoch number: 1601 and loss 162487.71875\n",
      "Epoch number: 1611 and loss 161612.5625\n",
      "Epoch number: 1621 and loss 160992.4375\n",
      "Epoch number: 1631 and loss 160630.328125\n",
      "Epoch number: 1641 and loss 160508.796875\n",
      "Epoch number: 1651 and loss 160063.390625\n",
      "Epoch number: 1661 and loss 159810.609375\n",
      "Epoch number: 1671 and loss 159071.0625\n",
      "Epoch number: 1681 and loss 159114.859375\n",
      "Epoch number: 1691 and loss 158194.640625\n",
      "Epoch number: 1701 and loss 157882.1875\n",
      "Epoch number: 1711 and loss 157927.421875\n",
      "Epoch number: 1721 and loss 156985.515625\n",
      "Epoch number: 1731 and loss 156664.015625\n",
      "Epoch number: 1741 and loss 155980.078125\n",
      "Epoch number: 1751 and loss 155761.8125\n",
      "Epoch number: 1761 and loss 155080.953125\n",
      "Epoch number: 1771 and loss 154243.09375\n",
      "Epoch number: 1781 and loss 153648.28125\n",
      "Epoch number: 1791 and loss 153831.046875\n",
      "Epoch number: 1801 and loss 153493.65625\n",
      "Epoch number: 1811 and loss 152643.1875\n",
      "Epoch number: 1821 and loss 152429.65625\n",
      "Epoch number: 1831 and loss 152249.671875\n",
      "Epoch number: 1841 and loss 151275.984375\n",
      "Epoch number: 1851 and loss 150907.90625\n",
      "Epoch number: 1861 and loss 150327.515625\n",
      "Epoch number: 1871 and loss 150859.90625\n",
      "Epoch number: 1881 and loss 150090.59375\n",
      "Epoch number: 1891 and loss 148797.734375\n",
      "Epoch number: 1901 and loss 148575.015625\n",
      "Epoch number: 1911 and loss 147941.359375\n",
      "Epoch number: 1921 and loss 148495.921875\n",
      "Epoch number: 1931 and loss 146885.953125\n",
      "Epoch number: 1941 and loss 146719.953125\n",
      "Epoch number: 1951 and loss 146472.5625\n",
      "Epoch number: 1961 and loss 145476.546875\n",
      "Epoch number: 1971 and loss 145595.6875\n",
      "Epoch number: 1981 and loss 145229.8125\n",
      "Epoch number: 1991 and loss 144730.609375\n",
      "Epoch number: 2001 and loss 143918.34375\n",
      "Epoch number: 2011 and loss 142945.34375\n",
      "Epoch number: 2021 and loss 142802.546875\n",
      "Epoch number: 2031 and loss 142685.234375\n",
      "Epoch number: 2041 and loss 142089.90625\n",
      "Epoch number: 2051 and loss 141075.921875\n",
      "Epoch number: 2061 and loss 140655.03125\n",
      "Epoch number: 2071 and loss 141348.828125\n",
      "Epoch number: 2081 and loss 139987.90625\n",
      "Epoch number: 2091 and loss 139168.484375\n",
      "Epoch number: 2101 and loss 139645.046875\n",
      "Epoch number: 2111 and loss 138414.03125\n",
      "Epoch number: 2121 and loss 137776.9375\n",
      "Epoch number: 2131 and loss 137511.078125\n",
      "Epoch number: 2141 and loss 137530.75\n",
      "Epoch number: 2151 and loss 136595.0625\n",
      "Epoch number: 2161 and loss 136075.296875\n",
      "Epoch number: 2171 and loss 135329.21875\n",
      "Epoch number: 2181 and loss 135828.328125\n",
      "Epoch number: 2191 and loss 134434.734375\n",
      "Epoch number: 2201 and loss 134941.296875\n",
      "Epoch number: 2211 and loss 134265.734375\n",
      "Epoch number: 2221 and loss 133744.296875\n",
      "Epoch number: 2231 and loss 133554.234375\n",
      "Epoch number: 2241 and loss 131905.59375\n",
      "Epoch number: 2251 and loss 132414.734375\n",
      "Epoch number: 2261 and loss 130829.046875\n",
      "Epoch number: 2271 and loss 132047.234375\n",
      "Epoch number: 2281 and loss 130553.84375\n",
      "Epoch number: 2291 and loss 130027.359375\n",
      "Epoch number: 2301 and loss 128843.34375\n",
      "Epoch number: 2311 and loss 128570.625\n",
      "Epoch number: 2321 and loss 128765.3515625\n",
      "Epoch number: 2331 and loss 128827.921875\n",
      "Epoch number: 2341 and loss 127329.4609375\n",
      "Epoch number: 2351 and loss 127125.2578125\n",
      "Epoch number: 2361 and loss 126495.609375\n",
      "Epoch number: 2371 and loss 125618.7890625\n",
      "Epoch number: 2381 and loss 125145.609375\n",
      "Epoch number: 2391 and loss 125046.9296875\n",
      "Epoch number: 2401 and loss 124332.984375\n",
      "Epoch number: 2411 and loss 123455.25\n",
      "Epoch number: 2421 and loss 122622.625\n",
      "Epoch number: 2431 and loss 122633.5390625\n",
      "Epoch number: 2441 and loss 122160.6171875\n",
      "Epoch number: 2451 and loss 121295.09375\n",
      "Epoch number: 2461 and loss 121542.796875\n",
      "Epoch number: 2471 and loss 121062.328125\n",
      "Epoch number: 2481 and loss 120869.8203125\n",
      "Epoch number: 2491 and loss 120260.8046875\n",
      "Epoch number: 2501 and loss 119967.796875\n",
      "Epoch number: 2511 and loss 118805.03125\n",
      "Epoch number: 2521 and loss 117753.8125\n",
      "Epoch number: 2531 and loss 118449.140625\n",
      "Epoch number: 2541 and loss 117522.3125\n",
      "Epoch number: 2551 and loss 115863.0546875\n",
      "Epoch number: 2561 and loss 116428.8984375\n",
      "Epoch number: 2571 and loss 117614.5078125\n",
      "Epoch number: 2581 and loss 114977.046875\n",
      "Epoch number: 2591 and loss 114769.34375\n",
      "Epoch number: 2601 and loss 113434.6640625\n",
      "Epoch number: 2611 and loss 113576.6796875\n",
      "Epoch number: 2621 and loss 113461.375\n",
      "Epoch number: 2631 and loss 111684.6953125\n",
      "Epoch number: 2641 and loss 111475.8046875\n",
      "Epoch number: 2651 and loss 111253.359375\n",
      "Epoch number: 2661 and loss 112421.6484375\n",
      "Epoch number: 2671 and loss 110959.3046875\n",
      "Epoch number: 2681 and loss 112159.6328125\n",
      "Epoch number: 2691 and loss 109303.4765625\n",
      "Epoch number: 2701 and loss 109501.453125\n",
      "Epoch number: 2711 and loss 108969.953125\n",
      "Epoch number: 2721 and loss 108782.25\n",
      "Epoch number: 2731 and loss 107785.375\n",
      "Epoch number: 2741 and loss 107531.34375\n",
      "Epoch number: 2751 and loss 105853.203125\n",
      "Epoch number: 2761 and loss 106466.59375\n",
      "Epoch number: 2771 and loss 105372.7734375\n",
      "Epoch number: 2781 and loss 105052.2109375\n",
      "Epoch number: 2791 and loss 104769.8828125\n",
      "Epoch number: 2801 and loss 104834.1171875\n",
      "Epoch number: 2811 and loss 103107.265625\n",
      "Epoch number: 2821 and loss 102696.5078125\n",
      "Epoch number: 2831 and loss 102459.125\n",
      "Epoch number: 2841 and loss 103300.953125\n",
      "Epoch number: 2851 and loss 101709.59375\n",
      "Epoch number: 2861 and loss 101445.9921875\n",
      "Epoch number: 2871 and loss 101629.6640625\n",
      "Epoch number: 2881 and loss 100509.1328125\n",
      "Epoch number: 2891 and loss 100271.6796875\n",
      "Epoch number: 2901 and loss 99627.171875\n",
      "Epoch number: 2911 and loss 100022.859375\n",
      "Epoch number: 2921 and loss 98935.0078125\n",
      "Epoch number: 2931 and loss 97141.7265625\n",
      "Epoch number: 2941 and loss 97459.984375\n",
      "Epoch number: 2951 and loss 96273.40625\n",
      "Epoch number: 2961 and loss 96075.53125\n",
      "Epoch number: 2971 and loss 96435.578125\n",
      "Epoch number: 2981 and loss 95371.7109375\n",
      "Epoch number: 2991 and loss 96190.2578125\n",
      "Epoch number: 3001 and loss 95644.328125\n",
      "Epoch number: 3011 and loss 96288.1953125\n",
      "Epoch number: 3021 and loss 93776.234375\n",
      "Epoch number: 3031 and loss 92815.125\n",
      "Epoch number: 3041 and loss 92179.140625\n",
      "Epoch number: 3051 and loss 91480.34375\n",
      "Epoch number: 3061 and loss 91956.296875\n",
      "Epoch number: 3071 and loss 90719.5859375\n",
      "Epoch number: 3081 and loss 90528.65625\n",
      "Epoch number: 3091 and loss 89970.6953125\n",
      "Epoch number: 3101 and loss 89691.1015625\n",
      "Epoch number: 3111 and loss 87987.75\n",
      "Epoch number: 3121 and loss 89028.4140625\n",
      "Epoch number: 3131 and loss 88355.0703125\n",
      "Epoch number: 3141 and loss 85323.8671875\n",
      "Epoch number: 3151 and loss 87322.046875\n",
      "Epoch number: 3161 and loss 86948.0078125\n",
      "Epoch number: 3171 and loss 86077.671875\n",
      "Epoch number: 3181 and loss 85493.71875\n",
      "Epoch number: 3191 and loss 87271.171875\n",
      "Epoch number: 3201 and loss 83479.6015625\n",
      "Epoch number: 3211 and loss 84235.0078125\n",
      "Epoch number: 3221 and loss 83737.359375\n",
      "Epoch number: 3231 and loss 82465.9140625\n",
      "Epoch number: 3241 and loss 82560.953125\n",
      "Epoch number: 3251 and loss 81449.390625\n",
      "Epoch number: 3261 and loss 82115.140625\n",
      "Epoch number: 3271 and loss 80628.21875\n",
      "Epoch number: 3281 and loss 80170.3203125\n",
      "Epoch number: 3291 and loss 80449.3671875\n",
      "Epoch number: 3301 and loss 80666.7734375\n",
      "Epoch number: 3311 and loss 79420.71875\n",
      "Epoch number: 3321 and loss 78006.1875\n",
      "Epoch number: 3331 and loss 78024.1015625\n",
      "Epoch number: 3341 and loss 78585.625\n",
      "Epoch number: 3351 and loss 76512.3515625\n",
      "Epoch number: 3361 and loss 76778.234375\n",
      "Epoch number: 3371 and loss 75783.5703125\n",
      "Epoch number: 3381 and loss 76200.2578125\n",
      "Epoch number: 3391 and loss 75534.1171875\n",
      "Epoch number: 3401 and loss 73653.6953125\n",
      "Epoch number: 3411 and loss 73275.359375\n",
      "Epoch number: 3421 and loss 73973.8046875\n",
      "Epoch number: 3431 and loss 73603.296875\n",
      "Epoch number: 3441 and loss 72536.421875\n",
      "Epoch number: 3451 and loss 72702.9921875\n",
      "Epoch number: 3461 and loss 74619.9765625\n",
      "Epoch number: 3471 and loss 71469.234375\n",
      "Epoch number: 3481 and loss 71394.21875\n",
      "Epoch number: 3491 and loss 70397.1015625\n",
      "Epoch number: 3501 and loss 69376.7578125\n",
      "Epoch number: 3511 and loss 69531.4296875\n",
      "Epoch number: 3521 and loss 68563.53125\n",
      "Epoch number: 3531 and loss 69446.921875\n",
      "Epoch number: 3541 and loss 68121.765625\n",
      "Epoch number: 3551 and loss 67769.703125\n",
      "Epoch number: 3561 and loss 66385.4140625\n",
      "Epoch number: 3571 and loss 66971.8125\n",
      "Epoch number: 3581 and loss 65896.828125\n",
      "Epoch number: 3591 and loss 64462.453125\n",
      "Epoch number: 3601 and loss 65851.546875\n",
      "Epoch number: 3611 and loss 63609.15625\n",
      "Epoch number: 3621 and loss 65315.94921875\n",
      "Epoch number: 3631 and loss 63965.3671875\n",
      "Epoch number: 3641 and loss 63295.890625\n",
      "Epoch number: 3651 and loss 63189.1484375\n",
      "Epoch number: 3661 and loss 64016.671875\n",
      "Epoch number: 3671 and loss 60491.3046875\n",
      "Epoch number: 3681 and loss 62643.91015625\n",
      "Epoch number: 3691 and loss 60907.0546875\n",
      "Epoch number: 3701 and loss 60039.85546875\n",
      "Epoch number: 3711 and loss 60454.26171875\n",
      "Epoch number: 3721 and loss 60168.91796875\n",
      "Epoch number: 3731 and loss 61118.421875\n",
      "Epoch number: 3741 and loss 58818.73046875\n",
      "Epoch number: 3751 and loss 57641.58203125\n",
      "Epoch number: 3761 and loss 59312.06640625\n",
      "Epoch number: 3771 and loss 57920.21484375\n",
      "Epoch number: 3781 and loss 56280.34765625\n",
      "Epoch number: 3791 and loss 56496.125\n",
      "Epoch number: 3801 and loss 57289.3046875\n",
      "Epoch number: 3811 and loss 55078.640625\n",
      "Epoch number: 3821 and loss 54758.8984375\n",
      "Epoch number: 3831 and loss 55457.4921875\n",
      "Epoch number: 3841 and loss 55001.265625\n",
      "Epoch number: 3851 and loss 55355.71875\n",
      "Epoch number: 3861 and loss 53918.22265625\n",
      "Epoch number: 3871 and loss 53620.51953125\n",
      "Epoch number: 3881 and loss 52114.34375\n",
      "Epoch number: 3891 and loss 51796.08203125\n",
      "Epoch number: 3901 and loss 52462.44140625\n",
      "Epoch number: 3911 and loss 51208.859375\n",
      "Epoch number: 3921 and loss 49906.48828125\n",
      "Epoch number: 3931 and loss 48859.32421875\n",
      "Epoch number: 3941 and loss 51266.07421875\n",
      "Epoch number: 3951 and loss 49034.8984375\n",
      "Epoch number: 3961 and loss 49075.02734375\n",
      "Epoch number: 3971 and loss 50737.40625\n",
      "Epoch number: 3981 and loss 49396.8828125\n",
      "Epoch number: 3991 and loss 48787.65625\n",
      "Epoch number: 4001 and loss 49597.18359375\n",
      "Epoch number: 4011 and loss 47774.74609375\n",
      "Epoch number: 4021 and loss 47931.58984375\n",
      "Epoch number: 4031 and loss 47188.00390625\n",
      "Epoch number: 4041 and loss 46162.21875\n",
      "Epoch number: 4051 and loss 45935.203125\n",
      "Epoch number: 4061 and loss 44610.8125\n",
      "Epoch number: 4071 and loss 48944.69140625\n",
      "Epoch number: 4081 and loss 46772.51953125\n",
      "Epoch number: 4091 and loss 43820.328125\n",
      "Epoch number: 4101 and loss 44329.69140625\n",
      "Epoch number: 4111 and loss 44301.390625\n",
      "Epoch number: 4121 and loss 43600.80859375\n",
      "Epoch number: 4131 and loss 43316.296875\n",
      "Epoch number: 4141 and loss 43948.859375\n",
      "Epoch number: 4151 and loss 43462.76171875\n",
      "Epoch number: 4161 and loss 44088.9765625\n",
      "Epoch number: 4171 and loss 42515.171875\n",
      "Epoch number: 4181 and loss 42863.12109375\n",
      "Epoch number: 4191 and loss 41855.0859375\n",
      "Epoch number: 4201 and loss 43032.4453125\n",
      "Epoch number: 4211 and loss 41808.62109375\n",
      "Epoch number: 4221 and loss 43073.05078125\n",
      "Epoch number: 4231 and loss 42264.02734375\n",
      "Epoch number: 4241 and loss 41568.78125\n",
      "Epoch number: 4251 and loss 41560.3984375\n",
      "Epoch number: 4261 and loss 41561.5859375\n",
      "Epoch number: 4271 and loss 41186.55078125\n",
      "Epoch number: 4281 and loss 40343.3984375\n",
      "Epoch number: 4291 and loss 42354.01953125\n",
      "Epoch number: 4301 and loss 41684.3125\n",
      "Epoch number: 4311 and loss 40443.33984375\n",
      "Epoch number: 4321 and loss 39587.91796875\n",
      "Epoch number: 4331 and loss 39111.59375\n",
      "Epoch number: 4341 and loss 41352.53125\n",
      "Epoch number: 4351 and loss 38195.0546875\n",
      "Epoch number: 4361 and loss 39790.546875\n",
      "Epoch number: 4371 and loss 39399.69921875\n",
      "Epoch number: 4381 and loss 40792.65625\n",
      "Epoch number: 4391 and loss 38218.64453125\n",
      "Epoch number: 4401 and loss 39485.44140625\n",
      "Epoch number: 4411 and loss 37974.28125\n",
      "Epoch number: 4421 and loss 38697.49609375\n",
      "Epoch number: 4431 and loss 38773.87890625\n",
      "Epoch number: 4441 and loss 37247.30078125\n",
      "Epoch number: 4451 and loss 38893.77734375\n",
      "Epoch number: 4461 and loss 37345.50390625\n",
      "Epoch number: 4471 and loss 38514.0234375\n",
      "Epoch number: 4481 and loss 37309.90234375\n",
      "Epoch number: 4491 and loss 39074.11328125\n",
      "Epoch number: 4501 and loss 35722.2421875\n",
      "Epoch number: 4511 and loss 36411.53125\n",
      "Epoch number: 4521 and loss 35884.8515625\n",
      "Epoch number: 4531 and loss 34896.15234375\n",
      "Epoch number: 4541 and loss 36185.2265625\n",
      "Epoch number: 4551 and loss 37331.1875\n",
      "Epoch number: 4561 and loss 37071.8984375\n",
      "Epoch number: 4571 and loss 37087.171875\n",
      "Epoch number: 4581 and loss 35563.74609375\n",
      "Epoch number: 4591 and loss 35632.36328125\n",
      "Epoch number: 4601 and loss 37102.41015625\n",
      "Epoch number: 4611 and loss 34328.40234375\n",
      "Epoch number: 4621 and loss 35325.33984375\n",
      "Epoch number: 4631 and loss 35314.80859375\n",
      "Epoch number: 4641 and loss 36555.953125\n",
      "Epoch number: 4651 and loss 39222.60546875\n",
      "Epoch number: 4661 and loss 38564.25390625\n",
      "Epoch number: 4671 and loss 36139.99609375\n",
      "Epoch number: 4681 and loss 36154.98046875\n",
      "Epoch number: 4691 and loss 37979.0234375\n",
      "Epoch number: 4701 and loss 36497.046875\n",
      "Epoch number: 4711 and loss 36677.8203125\n",
      "Epoch number: 4721 and loss 36240.2421875\n",
      "Epoch number: 4731 and loss 38380.75\n",
      "Epoch number: 4741 and loss 37560.75\n",
      "Epoch number: 4751 and loss 35164.703125\n",
      "Epoch number: 4761 and loss 35616.3203125\n",
      "Epoch number: 4771 and loss 36724.24609375\n",
      "Epoch number: 4781 and loss 36375.76171875\n",
      "Epoch number: 4791 and loss 35810.37109375\n",
      "Epoch number: 4801 and loss 35389.65625\n",
      "Epoch number: 4811 and loss 36049.7734375\n",
      "Epoch number: 4821 and loss 36005.9765625\n",
      "Epoch number: 4831 and loss 35090.56640625\n",
      "Epoch number: 4841 and loss 35336.640625\n",
      "Epoch number: 4851 and loss 35150.66015625\n",
      "Epoch number: 4861 and loss 36034.7734375\n",
      "Epoch number: 4871 and loss 34341.375\n",
      "Epoch number: 4881 and loss 34604.53515625\n",
      "Epoch number: 4891 and loss 35829.31640625\n",
      "Epoch number: 4901 and loss 34836.765625\n",
      "Epoch number: 4911 and loss 35916.70703125\n",
      "Epoch number: 4921 and loss 36660.71875\n",
      "Epoch number: 4931 and loss 36825.37890625\n",
      "Epoch number: 4941 and loss 35120.98828125\n",
      "Epoch number: 4951 and loss 35778.8984375\n",
      "Epoch number: 4961 and loss 34090.78515625\n",
      "Epoch number: 4971 and loss 35041.05859375\n",
      "Epoch number: 4981 and loss 37140.90625\n",
      "Epoch number: 4991 and loss 33525.328125\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "epochs=5000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    y_pred=model(train_categorical,train_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,ytrain)) ## RMSE\n",
    "    final_losses.append(loss.item())\n",
    "    if i%10==1:\n",
    "        print(f\"Epoch number: {i} and loss {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d400d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAGwCAYAAACAZ5AeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhnklEQVR4nO3deVhTZ9oG8DsBEnAB3CCgCLhvuCti1WplDEoXW6dVa1vb2kWLTl2qaK1rrTA6tmprtZ2Zqt3VftW2oljEhaqoFQVFhbqAuAVUJAFkz/v94XhqmgBBQxLg/l1Xrs+875OTJ2f8mtuTc94jE0IIEBEREdFDkdu6ASIiIqLagKGKiIiIyAIYqoiIiIgsgKGKiIiIyAIYqoiIiIgsgKGKiIiIyAIYqoiIiIgswNHWDdQler0e165dQ8OGDSGTyWzdDhEREZlBCIHc3Fx4e3tDLi//eBRDlRVdu3YNPj4+tm6DiIiIHsDly5fRokWLcucZqqyoYcOGAO7+j+Lq6mrjboiIiMgcOp0OPj4+0vd4eRiqrOjeT36urq4MVURERDVMZafu8ER1IiIiIgtgqCIiIiKyAIYqIiIiIgtgqCIiIiKyAIYqIiIiIgtgqCIiIiKyAIYqIiIiIgtgqCIiIiKyAIYqIiIiIgtgqCIiIiKyAIYqIiIiIguwaaiKiIhAnz590LBhQ3h4eGDkyJFITU01qCksLERYWBiaNGmCBg0aYNSoUcjMzDSoycjIQGhoKOrVqwcPDw/MnDkTpaWlBjX79u1Dz549oVQq0aZNG2zYsMGonzVr1sDPzw/Ozs4IDAzE0aNHq9wLERER1U02DVX79+9HWFgYDh8+jJiYGJSUlGDYsGHIz8+XaqZNm4ZffvkFW7Zswf79+3Ht2jU888wz0nxZWRlCQ0NRXFyMQ4cOYePGjdiwYQPmz58v1aSlpSE0NBRDhgxBYmIipk6ditdeew27du2SajZt2oTp06djwYIFOH78OLp16wa1Wo2srCyze7GVW3lFuHL7DrJ0hbidX4y8olIUlZZBrxe2bo2IiKjOkAkh7Oab98aNG/Dw8MD+/fsxaNAgaLVaNGvWDN9++y3+/ve/AwBSUlLQsWNHxMfHo1+/fti5cycef/xxXLt2DZ6engCAdevWITw8HDdu3IBCoUB4eDiioqKQnJwsvdeYMWOQk5OD6OhoAEBgYCD69OmDTz75BACg1+vh4+ODKVOmYPbs2Wb1UhmdTgc3NzdotVq4urpabL+9t+0Uvj6cYXLOUS6D0lEONxcnuNdTwL2eExr97/82qa+Al7sLWjauB59G9eDl7gwnB/4iTEREdD9zv78drdhTpbRaLQCgcePGAICEhASUlJQgODhYqunQoQNatmwpBZn4+HgEBARIgQoA1Go1Jk2ahNOnT6NHjx6Ij4832Ma9mqlTpwIAiouLkZCQgDlz5kjzcrkcwcHBiI+PN7uXvyoqKkJRUZH0XKfTPeiuqZCDTAZnJzlKygTK/nJ0qlQvUFpchvziMlzTFla8HbkM3u7OaNm4Hlo1bYC2ng3Q0csVHb1c0UBpV39ViIiI7I7dfFPq9XpMnToVjzzyCLp06QIA0Gg0UCgUcHd3N6j19PSERqORau4PVPfm781VVKPT6VBQUIDbt2+jrKzMZE1KSorZvfxVREQEFi1aZOYeeHCLnuqCRU/d3WdleoGSMv3/Hnf/XFBcBm1BCW7fKUbOnRLk3CnG7TsluJlXhGs5Bbh8uwAZ2XdQXKrH5ewCXM4uwMHztwzeo0UjF3Rt4YaeLRuht19jdPRqCKWjQ7V/NiIioprCbkJVWFgYkpOTceDAAVu3YjFz5szB9OnTpec6nQ4+Pj7V+p4Ochkc5A5wdqpa4NHrBW7kFSEj+w7Sb+bjwo18nMvMRfI1LTJ1RbhyuwBXbhdgx6m7AVLpePdnQnVnFZ7u0RyBrRqjnsJu/joRERFZnV18C06ePBnbt29HXFwcWrRoIY2rVCoUFxcjJyfH4AhRZmYmVCqVVPPXq/TuXZF3f81fr9LLzMyEq6srXFxc4ODgAAcHB5M192+jsl7+SqlUQqlUVmFP2I5cLoOnqzM8XZ3Rx6+xwVzOnWKcuaZD4pUc/J6WjcTLObh9pwQA8HPSNfycdA0KBzl6+zVC/9ZN0Mu3Mfq1agyZTGaLj0JERGQTNg1VQghMmTIFW7duxb59++Dv728w36tXLzg5OSE2NhajRo0CAKSmpiIjIwNBQUEAgKCgIHzwwQfIysqCh4cHACAmJgaurq7o1KmTVLNjxw6DbcfExEjbUCgU6NWrF2JjYzFy5EgAd3+OjI2NxeTJk83upbZyr6dA/zZN0b9NU2Dw3f/dLtzIxy9J13Dicg4uZOXhak4BDl24hUMX7v5s6NFQicHtm2F4Fy880qYpFI48AZ6IiGo3m17999Zbb+Hbb7/FTz/9hPbt20vjbm5ucHFxAQBMmjQJO3bswIYNG+Dq6oopU6YAAA4dOgTg7pIK3bt3h7e3N5YtWwaNRoMXX3wRr732GpYuXQrg7pIKXbp0QVhYGF599VXs2bMH//jHPxAVFQW1Wg3g7pIK48ePx2effYa+ffti5cqV2Lx5M1JSUqRzrSrrpTLVdfWfrQkhkHYzH3F/3ED0aQ0OX8w2mG/o7IjcwlK8PtAfM4a1r/JPk0RERLZk9ve3sCEAJh/r16+XagoKCsRbb70lGjVqJOrVqyeefvppcf36dYPtpKeni+HDhwsXFxfRtGlTMWPGDFFSUmJQs3fvXtG9e3ehUChEq1atDN7jno8//li0bNlSKBQK0bdvX3H48GGDeXN6qYhWqxUAhFarNfs1NVFBcanYn5ol3tt6SvR6P0b4hm+XHt0W7RILf04Wxy9lC71eb+tWiYiIKmXu97ddrVNV29XWI1UVKSnTY09KFt78KsForr7CAW4uTlj6TAAGt/ewQXdERESVM/f7m6HKiupiqLpfmV5g/x9Z2HbiGnafzcSd4jKjmjOL1byKkIiI7ApDlR2q66HqfoUlZfj2SAYWbz9jNDe2b0uM7O6NwFZNbNAZERGRIYYqO8RQZdqPx69g+uYkk3PT/9YOEx9tzasHiYjIZhiq7BBDVcVKy/T47fxNvPvjKVw3cUudUwuHoaGzkw06IyKiuoyhyg4xVJnvl6RrmPLdCZNza8f1REgXFRcXJSIiq2CoskMMVVWXqsmFemWcybn3n+qMF4P8rNsQERHVOQxVdoih6sHpCksw6tNDOJeVZzQ3d0RHvDrAHw5yHrkiIiLLY6iyQwxVD6+0TI9eS3ZDW1BiNDfx0dYID2nPnwWJiMiiGKrsEEOV5ZSW6TF+/VEcPH/LaG712B54spu3DboiIqLaiKHKDjFUWV5BcRmGr4pD+q07RnMp74fwPoNERPTQGKrsEENV9Skt02PCxmPY/8cNo7ltYY+gu4+79ZsiIqJagaHKDjFUVb/SMj3azN1pNN5Q6Yj9s4agcX2FDboiIqKazNzvby5TTbWKo4Mc6ZGh+GpCX4Px3KJS9Hw/Bmv2ngf/HUFERNWBR6qsiEeqrO/39Gw8uy7eaDzimQCM7dvSBh0REVFNw5//7BBDle18uu88lkWnGo3vnzkYvk3q26AjIiKqKRiq7BBDlW0VlpShw7xok3PnPxgORwf+Gk5ERMZ4ThXRXzg7OSA9MhQ73x5oNNdm7k4s/Pk0yvT8NwYRET0YHqmyIh6psh9CCOxNzcKrG44ZzZ1dHAIXBde3IiKiu3ikiqgCMpkMj3XwxKHZjxnNdZwfjVk/JPEqQSIiqhKGKqrTvN1dkB4Zil8mDzAY33zsCvzn7MCd4lIbdUZERDUNQxURgIAWbkiPDIXS0fD/JTrN34Xn1sWjuFRvo86IiKimYKgiuk/qkuG4uHSEwdjR9Gy0e28nNNpCG3VFREQ1AUMV0V/I5TKkR4YiNMDLYLxfRCzavbcTpWU8akVERMYYqojKsWZcT/yxZLjBWHHp3XsLRp28bqOuiIjIXjFUEVVA4Xj3XoKdvQ0voQ379jhGrjnIo1ZERCRhqCIyQ9Q/BuLMYrXBWOLlHLSZuxPns3Jt1BUREdkThioiM9VTOCI9MhRzR3Q0GA/+MA5fHEizUVdERGQvGKqIquj1Qa2wfYrhulaLt5+B3+wo3MorslFXRERkawxVRA+gS/O761otfTrAYLzXkt04cO4mV2MnIqqDGKqIHsLzgS3xxcu9DcZe+O8R+M/ZAV1hiY26IiIiW2CoInpIj3XwROqSEKPxrgt/xaVb+TboiIiIbIGhisgClI4OSI8MxeQhbQzGH12+D36zo6DX8+dAIqLajqGKyILeUbdHyvvGR61avbsDZQxWRES1GkMVkYU5Ozng/AfDjcZbv7sDeUWlNuiIiIisgaGKqBo4OshxcekITBrc2mC8y4Jd2HVaY6OuiIioOjFUEVUTuVyG8JAO+GWy4ZpWb36VgOmbEm3TFBERVRuGKqJqFtDi7ppWbw5qJY39eOIq/GZH4UYuFwslIqotGKqIrGTOiI54/6nOBmN9PtiN745m2KgjIiKyJIYqIit6McgP/37JcLHQOT+ewqi1h6DRFtqoKyIisgSbhqq4uDg88cQT8Pb2hkwmw7Zt2wzmZTKZycfy5culGj8/P6P5yMhIg+2cPHkSAwcOhLOzM3x8fLBs2TKjXrZs2YIOHTrA2dkZAQEB2LFjh8G8EALz58+Hl5cXXFxcEBwcjHPnzlluZ1Cd8bdOnvh12iCDsYRLt9EvItZGHRERkSXYNFTl5+ejW7duWLNmjcn569evGzy++OILyGQyjBo1yqBu8eLFBnVTpkyR5nQ6HYYNGwZfX18kJCRg+fLlWLhwIT7//HOp5tChQxg7diwmTJiAEydOYOTIkRg5ciSSk5OlmmXLlmH16tVYt24djhw5gvr160OtVqOwkEcXqOraeTZEWsQIo3G/2VFIuJRtg46IiOhhyYSd3PlVJpNh69atGDlyZLk1I0eORG5uLmJj//wXvZ+fH6ZOnYqpU6eafM3atWsxd+5caDQaKBQKAMDs2bOxbds2pKSkAABGjx6N/Px8bN++XXpdv3790L17d6xbtw5CCHh7e2PGjBl45513AABarRaenp7YsGEDxowZY9Zn1Ol0cHNzg1arhaurq1mvodqtpEyPZ9fFI/FyjsH4+091xotBfjbpiYiIDJn7/V1jzqnKzMxEVFQUJkyYYDQXGRmJJk2aoEePHli+fDlKS/9cYDE+Ph6DBg2SAhUAqNVqpKam4vbt21JNcHCwwTbVajXi4+MBAGlpadBoNAY1bm5uCAwMlGpMKSoqgk6nM3gQ3c/JQY5tYY9gbF8fg/F5P53GxkPpvL0NEVENUmNC1caNG9GwYUM888wzBuP/+Mc/8P3332Pv3r148803sXTpUsyaNUua12g08PT0NHjNvecajabCmvvn73+dqRpTIiIi4ObmJj18fHzKraW6LeKZrjg6d6jB2IKfT6PVuzt4AjsRUQ1RY0LVF198gXHjxsHZ2dlgfPr06Rg8eDC6du2KiRMnYsWKFfj4449RVGT79X/mzJkDrVYrPS5fvmzrlsiOeTR0RnpkKHyb1DMY7xcRyyNWREQ1QI0IVb/99htSU1Px2muvVVobGBiI0tJSpKenAwBUKhUyMzMNau49V6lUFdbcP3//60zVmKJUKuHq6mrwIKpM1D8GQi4zHOMRKyIi+1cjQtV///tf9OrVC926dau0NjExEXK5HB4eHgCAoKAgxMXFoaSkRKqJiYlB+/bt0ahRI6nm/pPf79UEBQUBAPz9/aFSqQxqdDodjhw5ItUQWUoDpSMuRoTCv2l9g/F+EbH4OJbLeBAR2Subhqq8vDwkJiYiMTERwN0TwhMTE5GR8ecK0zqdDlu2bDF5lCo+Ph4rV65EUlISLl68iG+++QbTpk3DCy+8IAWm559/HgqFAhMmTMDp06exadMmrFq1CtOnT5e28/bbbyM6OhorVqxASkoKFi5ciGPHjmHy5MkA7l6ZOHXqVCxZsgQ///wzTp06hZdeegne3t4VXq1I9DD2vjMYM9XtDcZWxPyBx/61zzYNERFRxYQN7d27VwAweowfP16q+eyzz4SLi4vIyckxen1CQoIIDAwUbm5uwtnZWXTs2FEsXbpUFBYWGtQlJSWJAQMGCKVSKZo3by4iIyONtrV582bRrl07oVAoROfOnUVUVJTBvF6vF/PmzROenp5CqVSKoUOHitTU1Cp9Xq1WKwAIrVZbpddR3Xbmmlb4hm83euj1elu3RkRUJ5j7/W0361TVBVynih5UYUkZOsyLNhpPixgBmUxm4hVERGQptW6dKqK6zNnJAReXGq/A7j9nB/jvIiIi+8BQRVRDyOUypEeGGo37z9mBkjK9DToiIqL7MVQR1TBpESPQs6W7wVjbuTvx2sZjPGpFRGRDDFVENYxMJsOPbz1idGXg7rOZGL7qNxt1RUREDFVENVTYkDY4OPsxg7EUTS78Zkfh0q18G3VFRFR3MVQR1WDN3V2QuiTEaPzR5ftQyvOsiIisiqGKqIZTOjogacEwo/E2c3fiurbABh0REdVNDFVEtYCbi5PJJReCIvZg5e4/bNAREVHdw1BFVEvI5TKcXGh8xGrl7nMoLCmzQUdERHULQxVRLeLq7GRyLasO86KRceuODToiIqo7GKqIaiFTwWrQ8r3Ym5Jlg26IiOoGhiqiWipx/t8w//FOBmOvbPgdL/73iI06IiKq3RiqiGop93oKvDrAH7NCDBcJ/e3cTRw8f9NGXRER1V4MVUS13FuD2yBm2iCDsXH/OYLnPouHXs/b2hARWQpDFVEd0NazIaKnDjQYO5qWjVbv7kCmrtBGXRER1S4MVUR1RAeVK/5vUn+j8cClsViz97wNOiIiql0YqojqkF6+jZAeGYoFTxiewL58Vyo+jj1no66IiGoHhiqiOuiVR/yNxlbE/IGES9k26IaIqHZgqCKqo9IjQyGXGY6NWhuPiV8l2KYhIqIajqGKqA67GBGK4V1UBmPRpzXIKyq1UUdERDUXQxVRHffJ8z3RrKHSYKzLgl34JemajToiIqqZGKqI6jgHuQzxsx8zGp/y3Qn8ePyKDToiIqqZGKqICI4OcqRHhqKXbyOD8embk3Ai47aNuiIiqlkYqohIYmodq6c/PYSbeUU26IaIqGZhqCIiA6lLQozGei/ZzZPXiYgqwVBFRAaUjg5Ijww1Gu+yYBde2/i7DToiIqoZGKqIyKRvXw80Gtt9NgsLfkq2QTdERPZPJoTgbeqtRKfTwc3NDVqtFq6urrZuh8gsv6dn49l18QZjfk3q4ecpA+Dq7GSjroiIrMfc728eqSKiCvXxa4xFT3Y2GEu/dQeztpy0UUdERPaJoYqIKjW+vx++mtDXYCz6tAa38opQUqa3UVdERPaFoYqIzDKwbTP4NHYxGOu1ZDd6Lo6xUUdERPaFoYqIzBY3cwj6+jc2GMstKsVPiVdt1BERkf1gqCIis8lkMmx+MwjrXuhpMP7294l44uMDNuqKiMg+MFQRUZWFdPHCs71aGIyduqrFjM1JNuqIiMj2GKqI6IEsf7YbOnsbXlr8f8evIPmq1kYdERHZFkMVET2wzW8GGY09/vEBLP7lDLgEHhHVNQxVRPTA6isdkR4ZikfaNDEY/+JgGvzn7EB2frGNOiMisj6GKiJ6aN+81g+rxnQ3Gu/5PpdbIKK6g6GKiCziqe7N8VPYI0bjGw+lW78ZIiIbsGmoiouLwxNPPAFvb2/IZDJs27bNYP7ll1+GTCYzeISEhBjUZGdnY9y4cXB1dYW7uzsmTJiAvLw8g5qTJ09i4MCBcHZ2ho+PD5YtW2bUy5YtW9ChQwc4OzsjICAAO3bsMJgXQmD+/Pnw8vKCi4sLgoODce7cOcvsCKJaopuPO07M+5vB2IKfT+PIxVs26oiIyHpsGqry8/PRrVs3rFmzptyakJAQXL9+XXp89913BvPjxo3D6dOnERMTg+3btyMuLg5vvPGGNK/T6TBs2DD4+voiISEBy5cvx8KFC/H5559LNYcOHcLYsWMxYcIEnDhxAiNHjsTIkSORnJws1SxbtgyrV6/GunXrcOTIEdSvXx9qtRqFhYUW3CNENV+j+gp8/0Y/g7HRnx/G4x//hlLe0oaIajGZsJNLdGQyGbZu3YqRI0dKYy+//DJycnKMjmDdc/bsWXTq1Am///47evfuDQCIjo7GiBEjcOXKFXh7e2Pt2rWYO3cuNBoNFAoFAGD27NnYtm0bUlJSAACjR49Gfn4+tm/fLm27X79+6N69O9atWwchBLy9vTFjxgy88847AACtVgtPT09s2LABY8aMMeszmnuXa6LawG92lNFYv1aN8f0bxlcMEhHZM3O/v+3+nKp9+/bBw8MD7du3x6RJk3Dr1p8/I8THx8Pd3V0KVAAQHBwMuVyOI0eOSDWDBg2SAhUAqNVqpKam4vbt21JNcHCwwfuq1WrEx8cDANLS0qDRaAxq3NzcEBgYKNWYUlRUBJ1OZ/AgqisuLh2B9a/0MRg7fDEbV27fsVFHRETVy65DVUhICL788kvExsbin//8J/bv34/hw4ejrKwMAKDRaODh4WHwGkdHRzRu3BgajUaq8fT0NKi597yymvvn73+dqRpTIiIi4ObmJj18fHyq9PmJajK5XIYh7T0QHtLBYHzAP/ci/IeTNuqKiKj6ONq6gYrc/7NaQEAAunbtitatW2Pfvn0YOnSoDTszz5w5czB9+nTpuU6nY7CiOmfS4Nbo37oJnlpzUBrbdOwyPFyVmDGsvQ07IyKyLLs+UvVXrVq1QtOmTXH+/HkAgEqlQlZWlkFNaWkpsrOzoVKppJrMzEyDmnvPK6u5f/7+15mqMUWpVMLV1dXgQVQXdfNxxwdPdzEY+3jPeSz+5YyNOiIisrwaFaquXLmCW7duwcvLCwAQFBSEnJwcJCQkSDV79uyBXq9HYGCgVBMXF4eSkhKpJiYmBu3bt0ejRo2kmtjYWIP3iomJQVDQ3RNq/f39oVKpDGp0Oh2OHDki1RBRxcYF+uJfz3YzGPviYBp+Srxqo46IiCzLpqEqLy8PiYmJSExMBHD3hPDExERkZGQgLy8PM2fOxOHDh5Geno7Y2Fg89dRTaNOmDdRqNQCgY8eOCAkJweuvv46jR4/i4MGDmDx5MsaMGQNvb28AwPPPPw+FQoEJEybg9OnT2LRpE1atWmXws9zbb7+N6OhorFixAikpKVi4cCGOHTuGyZMnA7h7ZeLUqVOxZMkS/Pzzzzh16hReeukleHt7G1ytSEQV+3uvFgho7mYw9vb3ibZphojI0oQN7d27VwAweowfP17cuXNHDBs2TDRr1kw4OTkJX19f8frrrwuNRmOwjVu3bomxY8eKBg0aCFdXV/HKK6+I3Nxcg5qkpCQxYMAAoVQqRfPmzUVkZKRRL5s3bxbt2rUTCoVCdO7cWURFRRnM6/V6MW/ePOHp6SmUSqUYOnSoSE1NrdLn1Wq1AoDQarVVeh1RbdNt0S7hG77d4HE+K7fyFxIR2YC53992s05VXcB1qoj+1Ov9GNz6yw2X98x4FK2aNbBRR0REptWadaqIqHY6OjcYbi5OBmOPrdiP/KJSG3VERPRwGKqIyCYc5DIkLRhmNN55wS7sOHUdd4oZroioZmGoIiKb+npCoNHYW98cx+Dl+6zfDBHRQ2CoIiKb6t+6CcYFtjQaz8otwu2/nHNFRGTPGKqIyKbkchk+eDoA37xmfMSqx/sxuJlXZIOuiIiqjqGKiOzCI22aYu24nkbjvZfshl7Pi5SJyP4xVBGR3QjposKQ9s2Mxlu9uwP/+e2iDToiIjIfQxUR2Q2ZTIb1r/TFxEdbG80tiTprg46IiMzHUEVEdmf28A5oVM/JaDxg4S7+FEhEdouhiojs0t53BhuN5RaWotW7O5DHBUKJyA4xVBGRXXKvp0D8nMdMznVZsMvK3RARVY6hiojslpebC9IiRpicC1y6Gxm37li5IyKi8jFUEZFdk8lkOL1IbTSeqSvCoOV7bdAREZFpDFVEZPfqKx2RHhmKJ7t5G81dzubRKiKyDwxVRFRjrB7bA528XA3GBi7byxPXicguMFQRUY0S9Y8BRmNdFuxCxq07KCwps0FHRER3MVQRUY0ik8nw+kB/o/FBy/eiw7xo/JGZa4OuiIgYqoioBpob2gmH5ww1Offuj6es3A0R0V0MVURUI6ncnHH+g+FG48cu3cbJKznWb4iI6jyGKiKqsRwd5Nj59kCj8Sc/OYjiUr0NOiKiuoyhiohqtI5erpj/eCej8Xbv7USKRmeDjoiormKoIqIab3x/P/T1b2w0HrLyNxt0Q0R1FUMVEdV4DnIZvn+9n8m5MZ/HI/1mPoQQVu6KiOoahioiqhXkchl+nTbIaPzwxWwM/tc+fBTzhw26IqK6hKGKiGqNdp4NkR4ZipWjuxvNrd5znkeriKhaMVQRUa1j6h6BAOA/ZwfSbuZbuRsiqisYqoio1pHLZUiLGAGFo/F/4iZ9nWCDjoioLqhyqCooKMCdO3/eFf7SpUtYuXIlfv31V4s2RkT0MGQyGf5YYrw4aIomF90X/wq9nj8FEpFlVTlUPfXUU/jyyy8BADk5OQgMDMSKFSvw1FNPYe3atRZvkIjoYaQuCTEay7lTglbv7sDVnAKUMVwRkYVUOVQdP34cAwfeXcH4hx9+gKenJy5duoQvv/wSq1evtniDREQPQ+nogAtLR5iceyRyDwKXxuJ2frGVuyKi2qjKoerOnTto2LAhAODXX3/FM888A7lcjn79+uHSpUsWb5CI6GE5yGU4Z+I+gQBwM68Ik77heVZE9PCqHKratGmDbdu24fLly9i1axeGDRsGAMjKyoKrq6vFGyQisgQnBzkWP9XZ5Nzhi9nIuHXH5BwRkbmqHKrmz5+Pd955B35+fggMDERQUBCAu0etevToYfEGiYgs5aUgP7zc38/k3M7k69ZthohqHZl4gNXwNBoNrl+/jm7dukEuv5vLjh49CldXV3To0MHiTdYWOp0Obm5u0Gq1PKpHZEN+s6NMjv+xZLjJZRiIqG4z9/v7gf7roVKp0KNHD8jlcuh0Omzbtg0NGzZkoCKiGuG3WUNMjrd7byc+3Xfeyt0QUW1R5VD13HPP4ZNPPgFwd82q3r1747nnnkPXrl3xf//3fxZvkIjI0nwa10NaxAhETx1oNLcsOhUlZXobdEVENV2VQ1VcXJy0pMLWrVshhEBOTg5Wr16NJUuWWLxBIqLqIJPJ0EHlChcnB6O5tnN34lh6tg26IqKarMqhSqvVonHjxgCA6OhojBo1CvXq1UNoaCjOnTtn8QaJiKrTmcVqk/cK/Pu6eBt0Q0Q1WZVDlY+PD+Lj45Gfn4/o6GhpSYXbt2/D2dnZ4g0SEVUnmUyGj0Z3Nzn3r12p1m2GiGq0KoeqqVOnYty4cWjRogW8vb0xePBgAHd/FgwICLB0f0RE1c5BLsOkwa2Nxj/Zex638ops0BER1URVDlVvvfUW4uPj8cUXX+DAgQPSkgqtWrWq8jlVcXFxeOKJJ+Dt7Q2ZTIZt27ZJcyUlJQgPD0dAQADq168Pb29vvPTSS7h27ZrBNvz8/CCTyQwekZGRBjUnT57EwIED4ezsDB8fHyxbtsyoly1btqBDhw5wdnZGQEAAduzYYTAvhMD8+fPh5eUFFxcXBAcH8+dOolpkxt/amRzvtWS3lTshoprqgZZU6N27N55++mnUr18f95a5Cg0NxSOPPFKl7eTn56Nbt25Ys2aN0dydO3dw/PhxzJs3D8ePH8ePP/6I1NRUPPnkk0a1ixcvxvXr16XHlClTpDmdTodhw4bB19cXCQkJWL58ORYuXIjPP/9cqjl06BDGjh2LCRMm4MSJExg5ciRGjhyJ5ORkqWbZsmVYvXo11q1bhyNHjqB+/fpQq9UoLCys0mcmIvvk6CBHzLRBJuf8Zkfx/oBEVDnxADZu3Ci6dOkilEqlUCqVIiAgQHz55ZcPsikJALF169YKa44ePSoAiEuXLkljvr6+4qOPPir3NZ9++qlo1KiRKCoqksbCw8NF+/btpefPPfecCA0NNXhdYGCgePPNN4UQQuj1eqFSqcTy5cul+ZycHKFUKsV3331X7nsXFhYKrVYrPS5fviwACK1WW+HnJCLbOntdK3zDtxs9Ssv0tm6NiGxAq9Wa9f1d5SNVH374ISZNmoQRI0Zg8+bN2Lx5M0JCQjBx4kR89NFHls58BrRaLWQyGdzd3Q3GIyMj0aRJE/To0QPLly9HaWmpNBcfH49BgwZBoVBIY2q1Gqmpqbh9+7ZUExwcbLBNtVqN+Pi7V/+kpaVBo9EY1Li5uSEwMFCqMSUiIgJubm7Sw8fH54E/OxFZTweV6RWTW7+7A1uOXbZyN0RUUzhW9QUff/wx1q5di5deekkae/LJJ9G5c2csXLgQ06ZNs2iD9xQWFiI8PBxjx441WCL+H//4B3r27InGjRvj0KFDmDNnDq5fv44PP/wQwN1b6vj7+xtsy9PTU5pr1KgRNBqNNHZ/jUajkeruf52pGlPmzJmD6dOnS891Oh2DFVENsX/mYDy6fJ/R+MwfTqJHy0Zo7u4CF4XxGldEVHdVOVRdv34d/fv3Nxrv378/rl+vnhuSlpSU4LnnnoMQAmvXrjWYuz+0dO3aFQqFAm+++SYiIiKgVCqrpR9zKZVKm/dARA/Gt0l9xEwbhL99FGc0F/zhfvg0dkHczCGQyWQ26I6I7FGVf/5r06YNNm/ebDS+adMmtG3b1iJN3e9eoLp06RJiYmIqvRFxYGAgSktLkZ6eDuDufQozMzMNau49V6lUFdbcP3//60zVEFHt09azIX4KM30BzuXsAgz9cD/e/v6ElbsiIntV5VC1aNEizJ8/HyEhIXj//ffx/vvvIyQkBIsWLcLixYst2ty9QHXu3Dns3r0bTZo0qfQ1iYmJkMvl8PDwAAAEBQUhLi4OJSUlUk1MTAzat2+PRo0aSTWxsbEG24mJiUFQUBAAwN/fHyqVyqBGp9PhyJEjUg0R1U7dfNzxxcu9Tc5dvJGPnxKvmZwjorqnyqFq1KhROHLkCJo2bYpt27Zh27ZtaNq0KY4ePYqnn366StvKy8tDYmIiEhMTAdw9ITwxMREZGRkoKSnB3//+dxw7dgzffPMNysrKoNFooNFoUFx899Lm+Ph4rFy5EklJSbh48SK++eYbTJs2DS+88IIUmJ5//nkoFApMmDABp0+fxqZNm7Bq1SqDnw3ffvttREdHY8WKFUhJScHChQtx7NgxTJ48GcDdFZenTp2KJUuW4Oeff8apU6fw0ksvwdvbGyNHjqzqLiSiGuaxDp5IXqQud76UN2AmIuDBllSwlL179woARo/x48eLtLQ0k3MAxN69e4UQQiQkJIjAwEDh5uYmnJ2dRceOHcXSpUtFYWGhwfskJSWJAQMGCKVSKZo3by4iIyONetm8ebNo166dUCgUonPnziIqKspgXq/Xi3nz5glPT0+hVCrF0KFDRWpqapU+r7mXZBKRfbqVV2RyqYV/7jwrVsb8IQqKS23dIhFVA3O/v2VC/G/1zgrodDqzQ1pl5zzVZTqdDm5ubtBqtdxPRDVUxM6z+Gz/RZNzk4e0wTvq9lbuiIiqm7nf32Zd/efu7l7pFS5CCMhkMpSVlVWtUyKiGmTO8I5o0age5m1LNpo7nnHbBh0Rkb0wK1Tt3bu3uvsgIqoxXuzni+4t3PHEJwcMxg9duIWok9cxIkDFpRaI6iCzfv4jy+DPf0S1y+XsOxi4zPgfne+FdsRrA1vZoCMiqg7mfn8/0A2ViYgI8GlcD/83yXgx5CVRZ7Fm73kbdEREtsRQRUT0EHr5NsLbQ40XPl6+KxXFpVxqgaguYagiInpIkwa3Rn0T9wHceCgdAFCm51kWRHUBz6myIp5TRVS7+c2OKndu+5QB6NLczYrdEJGlWPycqqysrArnS0tLcfToUfM7JCKqZS4uHVHu3LRNidZrhIhswuxQ5eXlZRCsAgICcPnyZen5rVu3eB88IqrT5HIZTi9So7m7i9Hcuaw8G3RERNZkdqj666+E6enpBjcpNlVDRFTX1Fc6Yv/MwSbnruUUWLcZIrIqi56ozsXuiIgARwfT/2ntH7kHhSW86wRRbcWr/4iIqkHSgmF4ukdzo/EO86JxK6/IBh0RUXUzO1TJZDLk5uZCp9NBq9VCJpMhLy8POp1OehAR0V1uLk74aHR3nFmsNprrtWQ3Jn97nKdMENUyZt37D7h7vlS7du0Mnvfo0cPgOX/+IyIyVE9h+j+z209exwv9fNGvVRMrd0RE1cXsUMWbKhMRPZik+cPQbfGvRuPrD6aho5cr6ikc4FTOeVhEVHNw8U8r4uKfRHVbeYuDymRAWkSolbshInNZfPHP0tJSFBUZnlyZmZmJRYsWYdasWThw4MCDd0tEVAf8MnmAyXEhgM/2X0BeUamVOyIiSzL7SNUrr7wChUKBzz77DACQm5uLzp07o7CwEF5eXjhz5gx++uknjBhR/orCdR2PVBFRUWkZ2r8XXe78zrcHoqMX//tAZE8sfqTq4MGDGDVqlPT8yy+/RFlZGc6dO4ekpCRMnz4dy5cvf7iuiYhqOaWjA958tFW585/tv2DFbojIkswOVVevXkXbtm2l57GxsRg1ahTc3O7eIHT8+PE4ffq05TskIqpl3hrcpty5bYnX4Dc7Cno9T3clqmnMDlXOzs4oKPjzFguHDx9GYGCgwXxeHu9tRURUGTcXJxx5d2iFNbtOa6zUDRFZitmhqnv37vjqq68AAL/99hsyMzPx2GOPSfMXLlyAt7e35TskIqqFPF2dkR4ZivRI01f9bUu8auWOiOhhmR2q5s+fj1WrVqF169ZQq9V4+eWX4eXlJc1v3boVjzzySLU0SURUm5m6KnDX6UyUlOlt0A0RPSizF/989NFHkZCQgF9//RUqlQrPPvuswXz37t3Rt29fizdIRFTbBbRww7JRXTHr/04ajLeduxNb3+qPHi0b2agzIqoKLv5pRVxSgYgqUt7ioOX9REhE1mHu97fZR6ri4uLMqhs0aJC5myQiovv8sWQ4+kXEIju/2GA8v6gU9ZVm/+eaiGzE7CNVcrlcumFyeS+RyWQoKyuzXHe1DI9UEZE5TB2xSpo/DG71nGzQDRFZfPHPRo0awcfHB/PmzcO5c+dw+/Zto0d2drZFmiciqsv2vTPYaKzb4l8R98cN6zdDRGYzO1Rdv34d//znPxEfH4+AgABMmDABhw4dgqurK9zc3KQHERE9HL+m9U0Gq5e+OFruLwVEZHtmhyqFQoHRo0dj165dSElJQdeuXTF58mT4+Phg7ty5KC3ljUCJiCzFr2l9LH06wGh82qZE6zdDRGZ5qKv/0tLSMGHCBOzfvx83btxA48aNLdlbrcNzqoioqgYu24PL2QUGY56uSux8exAa11fYqCuiusXi51TdU1RUhG+//RbBwcHo0qULmjZtiqioKAYqIqJqsH3KQDT5S3jK1BWh5/sxuJFbZKOuiMgUs0PV0aNHMWnSJKhUKixfvhxPPvkkLl++jM2bNyMkJKQ6eyQiqrPcXJzw6zTTS9X0+WA3tHdKrNwREZWnSksqtGzZEuPHj0evXr3KrXvyySct1lxtw5//iOhBHUvPxt/XxZucO//BcDg6VPmHByIyk7nf31UKVZXhOlUVY6giooehKyxB14W/Go0/07M55o7oiCYNlDboiqj2s/g5VXq9vtIHAxURUfVxdXZC8iK10fiPx6+i15Ld+HTfeRt0RUT3WPR4cUFBQeVFRET0wBooHbHuBdOnYCyLTuU6VkQ2ZJFQVVRUhBUrVsDf398SmyMiogqEdFHBy83Z5Jz/nB3IL+K6gUS2YHaoKioqwpw5c9C7d2/0798f27ZtAwCsX78e/v7+WLlyJaZNm1ZdfRIR0X0WP9Wl3LnOC3ahTC+g1wvkMWARWY3ZoWr+/PlYu3Yt/Pz8kJ6ejmeffRZvvPEGPvroI3z44YdIT09HeHh4ld48Li4OTzzxBLy9vSGTyaSgdo8QAvPnz4eXlxdcXFwQHByMc+fOGdRkZ2dj3LhxcHV1hbu7OyZMmIC8vDyDmpMnT2LgwIFwdnaGj48Pli1bZtTLli1b0KFDBzg7OyMgIAA7duyoci9ERNYS3NED8x7vhH+/1Nvk/Pe/Z+C5z+LRZcEuXM6+Y+XuiOoms0PVli1b8OWXX+KHH37Ar7/+irKyMpSWliIpKQljxoyBg4NDld88Pz8f3bp1w5o1a0zOL1u2DKtXr8a6detw5MgR1K9fH2q1GoWFhVLNuHHjcPr0acTExGD79u2Ii4vDG2+8Ic3rdDoMGzYMvr6+SEhIwPLly7Fw4UJ8/vnnUs2hQ4cwduxYTJgwASdOnMDIkSMxcuRIJCcnV6kXIiJrkclkmDDAH3/r5Glyfu7WZBy7dBsA8H/Hr1izNaK6S5jJyclJXLlyRXru7OwsTp48ae7LKwVAbN26VXqu1+uFSqUSy5cvl8ZycnKEUqkU3333nRBCiDNnzggA4vfff5dqdu7cKWQymbh69aoQQohPP/1UNGrUSBQVFUk14eHhon379tLz5557ToSGhhr0ExgYKN58802zezGlsLBQaLVa6XH58mUBQGi12qrsGiKiSi34KVn4hm83+fjnzrMiJ7/Y1i0S1Vhardas72+zj1SVlZVBofjzVgmOjo5o0KCBxUPePWlpadBoNAgODpbG3NzcEBgYiPj4uwvgxcfHw93dHb17/3n4Ozg4GHK5HEeOHJFqBg0aZNC7Wq1Gamoqbt++LdXc/z73au69jzm9mBIREQE3Nzfp4ePj86C7g4ioQguf7Iz3n+pscu7TfRfQbfGvSL6qtXJXRHWLo7mFQgi8/PLLUCrvLi5XWFiIiRMnon79+gZ1P/74o0Ua02g0AABPT8ND256entKcRqOBh4eHwbyjoyMaN25sUPPXqxLvbVOj0aBRo0bQaDSVvk9lvZgyZ84cTJ8+XXqu0+kYrIio2owL9MW8n06XO/9Z3EV8PLaHFTsiqlvMDlXjx483eP7CCy9YvJnaRqlUSiGUiKi6yeUyrBrTHW9/n2hyvriUCzQTVSezQ9X69eursw8jKpUKAJCZmQkvLy9pPDMzE927d5dqsrKyDF5XWlqK7Oxs6fUqlQqZmZkGNfeeV1Zz/3xlvRAR2YOnujeHrqDE5BGrXaczUVhSBmenql9YRESVs9s7cPr7+0OlUiE2NlYa0+l0OHLkCIKCggAAQUFByMnJQUJCglSzZ88e6PV6BAYGSjVxcXEoKfnzTu4xMTFo3749GjVqJNXc/z73au69jzm9EBHZixeD/LD+5T4m5zrMi0ZBMY9YEVUHm4aqvLw8JCYmIjExEcDdE8ITExORkZEBmUyGqVOnYsmSJfj5559x6tQpvPTSS/D29sbIkSMBAB07dkRISAhef/11HD16FAcPHsTkyZMxZswYeHt7AwCef/55KBQKTJgwAadPn8amTZuwatUqg3Od3n77bURHR2PFihVISUnBwoULcezYMUyePBkAzOqFiMieDG7frNy5d7eeQpmet7MhsjjrXIxo2t69ewUAo8f48eOFEHeXMpg3b57w9PQUSqVSDB06VKSmphps49atW2Ls2LGiQYMGwtXVVbzyyisiNzfXoCYpKUkMGDBAKJVK0bx5cxEZGWnUy+bNm0W7du2EQqEQnTt3FlFRUQbz5vRSGXMvySQisoSv4tPLXWbh33EXRG5hici4lS/OZeZWvjGiOszc72+ZELz7prXodDq4ublBq9XC1dXV1u0QUS1XphfYeCgdDnIZFvxc/lWBAHBy4TC4OjtZqTOimsXc72+7PaeKiIgejoNchlcH+GN8f79Ka7N0RdXfEFEtx1BFRFQHbJ8yoMJ5B7nMSp0Q1V4MVUREdUCX5m5Ijwwtd/7+M0EidpzFaxuPQc+T2YmqhKGKiKgOOTxnqMnx7Sevo6RMD+Duyuu7z2ZKN2QmIvMwVBER1SEqN2e8P7KL0fiHMX/g+X8fNhi7F7KIyDwMVUREdcyL/Xzxn5d6G43/nn4bp6/9edNlnmVFVDUMVUREdVBwJ0+T46GrD/z5hKmKqEoYqoiI6qjj8/5W4byMqYqoShiqiIjqqMb1FbiwdARk5WSn3MIS0xNEZBJDFRFRHeYglyH1/eFQuTobzb3xVQIW/XJ3JfbcwhKcyLgN3oSDqHy8TY0V8TY1RGTPUjW5UK+MMxp3lMtQ+r81q1aN6Y6nuje3dmtENsXb1BARUZW0VzU0OV563yKgPx6/aq12iGochioiIpKse6GXrVsgqrEYqoiISBLSRVXh7Wz2/3HD4HkZb2VDJGGoIiIiI+XdzgYATl3R4rP9FxC5MwU9Fv+KS7fyrdgZkf3iiepWxBPViagmOXj+Jsb950ildU9288bqsT2s0BGRbfBEdSIieihBrZqYVXc+K6+aOyGqGRiqiIjIJLlchvMfDK+07sx1HX47d6PSOqLajqGKiIjK5eggNytYvfjfo1bohsi+MVQREVGFHB3kOLVwmK3bILJ7DFVERFSphs5OePPRVhXW3Fte4dsjGThw7qY12iKyK462boCIiGqGFu4uFc7fzCvCdW0h3t16CgAqXO+KqDZiqCIiIrM818cH57LyMLh9M3i6OiN09QGD+cClsXihX0sbdUdkewxVRERkFqWjAxY/1UV6PrZvS3x3NMOg5uvDGX99GVGdwXOqiIjogUQ8E4Df5wbbug0iu8FQRURED6xZQ2W5c5ez71ixEyLbY6giIqKH8tusISZXXx+4bC8KS8ps0BGRbTBUERHRQ/FpXA/fvdHP5FyHedG4kVtk5Y6IbIOhioiILCItYoTJ8T4f7EZpmd7K3RBZH0MVERFZhEwmK3dtqjZzd0L/v8VB/7UrFVO+OwEhhDXbI6p2MsG/1Vaj0+ng5uYGrVYLV1dXW7dDRFQtSsv0GLRsL65pC43mvNyccf1/4z++1R89WzaydntEVWbu9zePVBERkUU5OshxaM5QtPFoYDR3/b6gVVzKnwSpdmGoIiKiarF7+qMVzi/65Yz0kyBRbcBQRURE1earCX3LnTt7XYddpzVW7IaoejFUERFRtRnYthlS3g8pdz77TrEVuyGqXgxVRERUrZydHNCqWX2Tc3mFpVbuhqj6MFQREVG1e663j8nxiJ0p8JsdhU/2nLNyR0SWx1BFRETVbsIAfzjIZeXO/+vXP3Ai47YVOyKyPIYqIiKqdk4OcpxZrMamN/rhvdCOJmue/vQQjqZlW7kzIsux+1Dl5+cHmUxm9AgLCwMADB482Ghu4sSJBtvIyMhAaGgo6tWrBw8PD8ycOROlpYa/4+/btw89e/aEUqlEmzZtsGHDBqNe1qxZAz8/Pzg7OyMwMBBHjx6tts9NRFTbKB0dENiqCV4b2KrcmuhkXg1INZfdh6rff/8d169flx4xMTEAgGeffVaqef311w1qli1bJs2VlZUhNDQUxcXFOHToEDZu3IgNGzZg/vz5Uk1aWhpCQ0MxZMgQJCYmYurUqXjttdewa9cuqWbTpk2YPn06FixYgOPHj6Nbt25Qq9XIysqywl4gIqpdfps1xOT4FwfTcOX2HSt3Q2QZNe42NVOnTsX27dtx7tw5yGQyDB48GN27d8fKlStN1u/cuROPP/44rl27Bk9PTwDAunXrEB4ejhs3bkChUCA8PBxRUVFITk6WXjdmzBjk5OQgOjoaABAYGIg+ffrgk08+AQDo9Xr4+PhgypQpmD17tsn3LioqQlHRn3dn1+l08PHx4W1qiIgAfB53AUt3pJicK+8egkS2UCtvU1NcXIyvv/4ar776KmSyP094/Oabb9C0aVN06dIFc+bMwZ07f/4rJz4+HgEBAVKgAgC1Wg2dTofTp09LNcHBwQbvpVarER8fL71vQkKCQY1cLkdwcLBUY0pERATc3Nykh4+P6atfiIjqonGBvnBzcTI55zc7Ct8fzcD5rFyoP4rDz0nXrNwdUdU52rqBqti2bRtycnLw8ssvS2PPP/88fH194e3tjZMnTyI8PBypqan48ccfAQAajcYgUAGQnms0mgprdDodCgoKcPv2bZSVlZmsSUkx/a8sAJgzZw6mT58uPb93pIqIiID6SkckLRiG0jI92szdaTQ/+8dT0p//8d0JXMspgF+Tegjp4mXNNonMVqNC1X//+18MHz4c3t7e0tgbb7wh/TkgIABeXl4YOnQoLly4gNatW9uiTYlSqYRSqbRpD0RE9s7RwbwfTSJ33v1HLH8aJHtVY37+u3TpEnbv3o3XXnutwrrAwEAAwPnz5wEAKpUKmZmZBjX3nqtUqgprXF1d4eLigqZNm8LBwcFkzb1tEBHRg7uwdIStWyB6aDUmVK1fvx4eHh4IDa34XyiJiYkAAC+vu4eHg4KCcOrUKYOr9GJiYuDq6opOnTpJNbGxsQbbiYmJQVBQEABAoVCgV69eBjV6vR6xsbFSDRERPTgHuYzBimq8GhGq9Ho91q9fj/Hjx8PR8c9fLC9cuID3338fCQkJSE9Px88//4yXXnoJgwYNQteuXQEAw4YNQ6dOnfDiiy8iKSkJu3btwnvvvYewsDDpp7mJEyfi4sWLmDVrFlJSUvDpp59i8+bNmDZtmvRe06dPx7///W9s3LgRZ8+exaRJk5Cfn49XXnnFujuDiKiWcpDLEBpg/vlSQgiEfXMcS7afqcauiMxXI0LV7t27kZGRgVdffdVgXKFQYPfu3Rg2bBg6dOiAGTNmYNSoUfjll1+kGgcHB2zfvh0ODg4ICgrCCy+8gJdeegmLFy+Wavz9/REVFYWYmBh069YNK1aswH/+8x+o1WqpZvTo0fjXv/6F+fPno3v37khMTER0dLTRyetERPTglv29Kz57sRd6+zYqt+Zfu1JRWFKG09d0iDp1Hf85kGbFDonKV+PWqarJzF3ngoiI7i6rUJHVY3vgH9+dAMCT16l61cp1qoiIqO5IjwzFyYXDyp3/z28XpT9fuJGHguIya7RFVC6GKiIisluuzqYXBwWAk1e00p+HrtiPJz45YI2WiMrFUEVERHbt47E9zKo7n5VXzZ0QVYyhioiI7Nrg9s1s3QKRWRiqiIjIrjV0dsKXr/bFkpFdbN0KUYUYqoiIyO4NatesSmtYEdkCQxUREdUIjeor8M1rgdgysfw7WVzLKQAApGpyob1TYq3WiABwnSqr4jpVRESWceX2HQz4516Tc209GuBcVh4aOjvi1EK1yRqiquA6VUREVGu1aFSv3KsCz/3vKsDcwlLkFvJoFVkPQxUREdVIT3TzxroXelVYM+Rf+63UDRFDFRER1WAhXVT48Llu5c7fzCsyWHmdqDoxVBERUY32TM8WSJpf/u1slkSdtWI3VJcxVBERUY3nVs8JHg2V5c6XlOmt2A3VVQxVRERUK8TNGlLuXNu5O+E3OwqzfkiCRltoxa6oLmGoIiKiWsHZyQEHZz9WYc3mY1cw4J97rNQR1TUMVUREVGs0d3fB0XeHVlhTqufyjFQ9GKqIiKhW8XB1RuqSEOx9Z3C5NW9+dcx6DVGdwVBFRES1jtLRAf5N65d71GrX6Uzpz6euaDFq7SEcS8+2VntUSzFUERFRreXh6oyFT3QyOffNkUtYt/8Cnv70IBIu3cbf18UjK5cnsdODY6giIqJabWxgS5Pjc7cmI3JnisE5VjM2J1mrLaqFGKqIiKhWc5Kb/1V3IiOn+hqhWo+hioiIajW5XIavJvSFp2v5i4Pek1dUaoWOqLZiqCIiolpvYNtm2PdO+YuD3q+4lKuv04NhqCIiojrBReGA3dMHVVrX7r2dyC0ssUJHVNswVBERUZ3RxqMh0iND8Y/H2lRY969dqVbqiGoThioiIqpzpg9rj0VPdi53fmP8JdzMK7JiR1QbMFQREVGdNL6/H84uDil3vveS3cjScd0qMh9DFRER1VkuCgekR4aivsLB5HzfpbGIv3DLyl1RTcVQRUREdV7yIjUmDPA3OTflu+N4b9sp/JR41cpdUU3jaOsGiIiIbE0mk+G90I7Iyi3CL0nXDOZu5hXj68MZ+PpwBi7cyEe/Vo3RtYU7Gij5FUqGeKSKiIgId4PVB093qbBmdew5PP/vIxj/xVErdUU1CUMVERHR/7g6O+GFfqbvFXi/hEu3rdAN1TQMVURERPdZMjIAJxcOq7ROCFFpDdUtDFVERER/4ershLAhrSus8Z+zA18fvmSljqgmkAlGbavR6XRwc3ODVquFq6urrdshIqJKlOkFvjuagfe2JVdYF9JZhbUv9IRMJrNSZ2RN5n5/80gVERFRORzkMozu44N2ng0qrIs+rcHus1lG43eKS7H+YBqu3L5TXS2SHWGoIiIiqoCTgxy7pg7CkXeHVlj3+pfH8Nu5GwCAkjI9cu4UI3JnChb9cgZPfnLQGq2SjTFUERERVUImk8HT1RmfPN+jwrrZ/3cKXx++hCH/2ofui2Pw7ZEMAEB2frFUk1dUipfXH8WWY5ertWeyPoYqIiIiMz3e1Rt7Zjxa7vzVnAK8ty0ZV24XAABK9canLX8edxH7Um9g5g8nq61Psg27DlULFy6ETCYzeHTo0EGaLywsRFhYGJo0aYIGDRpg1KhRyMzMNNhGRkYGQkNDUa9ePXh4eGDmzJkoLS01qNm3bx969uwJpVKJNm3aYMOGDUa9rFmzBn5+fnB2dkZgYCCOHuXCb0REdVGrZg3Q3N3lgV+fc6e48iKqkew6VAFA586dcf36delx4MABaW7atGn45ZdfsGXLFuzfvx/Xrl3DM888I82XlZUhNDQUxcXFOHToEDZu3IgNGzZg/vz5Uk1aWhpCQ0MxZMgQJCYmYurUqXjttdewa9cuqWbTpk2YPn06FixYgOPHj6Nbt25Qq9XIyjI+KZGIiGq//TMHY2R3bygc7P5rlKzIrpdUWLhwIbZt24bExESjOa1Wi2bNmuHbb7/F3//+dwBASkoKOnbsiPj4ePTr1w87d+7E448/jmvXrsHT0xMAsG7dOoSHh+PGjRtQKBQIDw9HVFQUkpP/vFx2zJgxyMnJQXR0NAAgMDAQffr0wSeffAIA0Ov18PHxwZQpUzB79uxy+y8qKkJRUZH0XKfTwcfHh0sqEBHVImv2nsfyXamV1vX2bYQX+vki4dJtfPW/9a3SI0Oruz2ygFqzpMK5c+fg7e2NVq1aYdy4ccjIuHvSX0JCAkpKShAcHCzVdujQAS1btkR8fDwAID4+HgEBAVKgAgC1Wg2dTofTp09LNfdv417NvW0UFxcjISHBoEYulyM4OFiqKU9ERATc3Nykh4+Pz0PsCSIiskevD2xlVt2xS7cxdVMiBOz2WAY9JLsOVYGBgdiwYQOio6Oxdu1apKWlYeDAgcjNzYVGo4FCoYC7u7vBazw9PaHRaAAAGo3GIFDdm783V1GNTqdDQUEBbt68ibKyMpM197ZRnjlz5kCr1UqPy5d5pQcRUW2jcJTj/ZEV34i5PJt/v4xrOQXlzpfpBUrK9A/aGlmZo60bqMjw4cOlP3ft2hWBgYHw9fXF5s2b4eLy4CcJWotSqYRSqbR1G0REVM1G9/bBvEpWXb/n68MZ0p9n/d9JNHR2xKmFamlswU/JcFE4IjykPYavioO2oAQHwh+DE8/fsns16n8hd3d3tGvXDufPn4dKpUJxcTFycnIMajIzM6FSqQAAKpXK6GrAe88rq3F1dYWLiwuaNm0KBwcHkzX3tkFERHWbwlGOd4a1w+Qhbar82tzCP69Iv5ZTgI3xl7Bu/wUUlerxR2YeMnVFuHQr35LtUjWpUaEqLy8PFy5cgJeXF3r16gUnJyfExsZK86mpqcjIyEBQUBAAICgoCKdOnTK4Si8mJgaurq7o1KmTVHP/Nu7V3NuGQqFAr169DGr0ej1iY2OlGiIiosmPtcU76vYP9Nq8orvB6vO4i9LY/ZeR2e8lZXQ/uw5V77zzDvbv34/09HQcOnQITz/9NBwcHDB27Fi4ublhwoQJmD59Ovbu3YuEhAS88sorCAoKQr9+/QAAw4YNQ6dOnfDiiy8iKSkJu3btwnvvvYewsDDpZ7mJEyfi4sWLmDVrFlJSUvDpp59i8+bNmDZtmtTH9OnT8e9//xsbN27E2bNnMWnSJOTn5+OVV16xyX4hIiL7tXv6IGyZGISuLdzMfk2XBbuQlVuIDYfSpbFSPc+lqmns+pyqK1euYOzYsbh16xaaNWuGAQMG4PDhw2jWrBkA4KOPPoJcLseoUaNQVFQEtVqNTz/9VHq9g4MDtm/fjkmTJiEoKAj169fH+PHjsXjxYqnG398fUVFRmDZtGlatWoUWLVrgP//5D9TqP3/fHj16NG7cuIH58+dDo9Gge/fuiI6ONjp5nYiIqI1HQwDAlolB6PX+bukoVGX6fmD4q0nZfauxy2SW64+qj12vU1XbmLvOBRER1Q6FJWXoMC/6gV777ogOWLojBcDdo1/3wlpVfHPkEjYfu4L/ju+Npg144dSDqjXrVBEREdVUzk4OOD7vb0iaPwydvav2j+l7gQp48HOq5m5NRtLlHHwU88eDbYCqhKGKiIioGjWur4BbPSd8PSEQ7wxr90Db2HXaeF3E7Pxi7D6TiVIz1rHKN/MnSHo4DFVERERW0Ki+ApMfa4u97wyu8mu///0ycu4U44/MXNw7a2fkmoN47ctjBie3l2f7yevQFZZU+X2pahiqiIiIrMi/aX3Me7xTlV5z5XYBui+OwbCP4rAv9QYAICP7DgBgx6nrlb6+VC8w6euEqjdLVcJQRUREZGUTBvgjdsajOLs4pMqvfWXD7wa3rsnUFeHnpGuV/gx48PytKr8XVQ1DFRERkQ20btYALgoHTAuu+nlWbefulP58NacA//juBNrM3Ynfzt2wZItURQxVRERENvR2cFskL1JjaAePh97Wi/89arC+1V+Zc1I7PTi7XvyTiIioLmigdMR/X+4DAPCbHfVQ2wpd/Rsu3sxHcalxgPrxxFWkXM/Fc31aoIPKcImHO8Wl2HjoEtSdPdGqWYOH6qGu4pEqIiIiO/T+yC54ukfzKr8uRZNrMlABwKwfTuKLg2kIWfkbopM1yLh1R5pbFp2Kf0an4LEV+81+rx+PX8GBczer3GNtxSNVREREduS3WUNw+poW6s4qFJWUYeuJq9XyPhP/dzVgemQoACDh0m1prqC4DC4Khwpffz4rD9M3Jxlso67jkSoiIiI74tO4HkK6eEEmk+HFIF8M76Kq1vfb+b8lGe6/v+BTaw5U+rqs3EKz30MIgZgzmdBozX9NTcRQRUREZKeUjg5Y83xP6fljFjiZ/a8mfXMcN/OKcP89m//IzDOo+eJAGoIiYpF+M18ac5T/GSFM3Ua4oLhM+vPPSdfw+pfH8Mg/95Tbh66wBO9vP4Okyzkm50vL9BUGuVt5RTh88ZbJXqyFoYqIiMiOyeUyHHsvGEffHYr3R3ZBB1XVb6xcmZt5RYaHqgDo/3cV4fmsPCzefgbXtYWY9M1xad5B/md96f9qhRAo0wu8/f0JdJwfjRMZt/FVfDr+/dtFAKjwysTl0an474E0PLXmoMH4vdf8fV08+n4Qi9PXtCZf/9iK/Rjz+WH8eibT3I9tcTynioiIyM41baCU/vzT5EfQ/r1o6bnSUY6ick5MN1fIyt+Mxj6M+QPvqNsj+MM/T1w/e12H6ZsScfjiLUx+rK00Xlom4CATePazeINzs57+9JDZPaRm5hqNJV7Owbh/H0b48A5I/N8RrB+PX0VnbzejWm3B3dvw7E3Jgrpz9f5kWh6GKiIiohpE6eiA+DmP4VpOIRooHdFe1fChl2Ew5ZO957H/D+PFRH/834nz7249JY0Vl+pxNafAIFBVlVxmPDZ9cyLyi8sw/6fT0piJMumoGgDIZKYqrIOhioiIqIbxcnOBl5uL9DxpwTD0ej9G+hnOUk5dNf1T2191W/yr2ds8fU0rHWkSQiDtZj78m9aH/L4wJIS4G45MfJy/ZqaPYv7Al/Hp0nNT4cxaGKqIiIhqODcXJ/yxZDjmbjuFgObuuHz7Dtbuu2DrtkwKXX0A8XMeQ/rNO/hgxxkkX9XhxX6++OO+n/8OXbiFHi3dcfG+E+Pvkf8lVa2KPWfw3MGGqYqhioiIqBaQy2WIeKar9Dw8pANCV/+G09d0NuzKtKAIw6sAvzp8yeD51ZwC/FbOoqKleoGs3EJ4NHQ2Of/X0GVNDFVERES1VG/fRlKoSo8MrZZzr6rDrB9Ooq2H6Vvl/PdAGv57IA3A3SN0f2XDTMVQRUREVFvNDOmAZg2VCOniBQBY/0ofvLL+dxt3ZZ5zWXmV1ty74u9+DjZMVVynioiIqJZqoHTE5Mfaos3/jvoMae+Bn8IeQZP6CjzarplBXW0h5zlVREREZA3dfNxx7L1gaemBK7fvwMvNBTO3JOHHE1exfcoAPP5x5bepsVe2/PmPR6qIiIjqmPvXcmrRqB4c5DJ8OLo70iND0aX5nwtrvj+yi8Hr5j3eyWo9PqhA/8Y2e28eqSIiIiIDi5/qjOvaQrzYzxcuTg6I++MGVjzXDU4Ocgzr5ImjadnY98cN/JJ0DQAQ1KoJ4i/esnHXdw1q26zyomoiE7a882Ado9Pp4ObmBq1WC1dXV1u3Q0RE9FCikzX4z28X8dHo7tAVliB0te1/NkyPDLX4Ns39/uaRKiIiInogIV1UCOny5332LiwdgTPXdPByd0bvJbsBAM/2aoEtCVcAAG8PbSst1hk741Fk6grRtYU7tp64ios38hDSWYWFv5zBU929Ebkzxfof6CExVBEREZFFOMhlCGhx95ysrW/1R32lI76+b2HPSYNbo7CkDH/r5InWzRqgdbO7VyW+2M9Xqtn59kAAQB+/Rhi1Nl4af3dEByzdkYKBbZviqe7N0bOlO1o1a4DcwhKs+PUP7E3Nwix1B2t8zHLx5z8r4s9/RERU19zILcLz/z6M0X188NrAVlV6bcyZTLy//QxWjumOni0bVVOHlTP3+5uhyooYqoiIiGoec7+/uaQCERERkQUwVBERERFZAEMVERERkQUwVBERERFZAEMVERERkQUwVBERERFZAEMVERERkQUwVBERERFZAEMVERERkQUwVBERERFZgF2HqoiICPTp0wcNGzaEh4cHRo4cidTUVIOawYMHQyaTGTwmTpxoUJORkYHQ0FDUq1cPHh4emDlzJkpLSw1q9u3bh549e0KpVKJNmzbYsGGDUT9r1qyBn58fnJ2dERgYiKNHj1r8MxMREVHNZNehav/+/QgLC8Phw4cRExODkpISDBs2DPn5+QZ1r7/+Oq5fvy49li1bJs2VlZUhNDQUxcXFOHToEDZu3IgNGzZg/vz5Uk1aWhpCQ0MxZMgQJCYmYurUqXjttdewa9cuqWbTpk2YPn06FixYgOPHj6Nbt25Qq9XIysqq/h1BREREdq9G3VD5xo0b8PDwwP79+zFo0CAAd49Ude/eHStXrjT5mp07d+Lxxx/HtWvX4OnpCQBYt24dwsPDcePGDSgUCoSHhyMqKgrJycnS68aMGYOcnBxER0cDAAIDA9GnTx988sknAAC9Xg8fHx9MmTIFs2fPNqt/3lCZiIio5qmVN1TWarUAgMaNGxuMf/PNN2jatCm6dOmCOXPm4M6dO9JcfHw8AgICpEAFAGq1GjqdDqdPn5ZqgoODDbapVqsRHx8PACguLkZCQoJBjVwuR3BwsFRjSlFREXQ6ncGDiIiIaidHWzdgLr1ej6lTp+KRRx5Bly5dpPHnn38evr6+8Pb2xsmTJxEeHo7U1FT8+OOPAACNRmMQqABIzzUaTYU1Op0OBQUFuH37NsrKykzWpKSklNtzREQEFi1aZDTOcEVERFRz3PveruzHvRoTqsLCwpCcnIwDBw4YjL/xxhvSnwMCAuDl5YWhQ4fiwoULaN26tbXbNDBnzhxMnz5den716lV06tQJPj4+NuyKiIiIHkRubi7c3NzKna8RoWry5MnYvn074uLi0KJFiwprAwMDAQDnz59H69atoVKpjK7Sy8zMBACoVCrp/94bu7/G1dUVLi4ucHBwgIODg8mae9swRalUQqlUSs8bNGiAy5cvo2HDhpDJZJV8avPpdDr4+Pjg8uXLPFermnFfWwf3s3VwP1sH97N1VOd+FkIgNzcX3t7eFdbZdagSQmDKlCnYunUr9u3bB39//0pfk5iYCADw8vICAAQFBeGDDz5AVlYWPDw8AAAxMTFwdXVFp06dpJodO3YYbCcmJgZBQUEAAIVCgV69eiE2NhYjR44EcPfnyNjYWEyePNnszyOXyysNhQ/D1dWV/w9rJdzX1sH9bB3cz9bB/Wwd1bWfKzpCdY9dh6qwsDB8++23+Omnn9CwYUPpHCg3Nze4uLjgwoUL+PbbbzFixAg0adIEJ0+exLRp0zBo0CB07doVADBs2DB06tQJL774IpYtWwaNRoP33nsPYWFh0lGkiRMn4pNPPsGsWbPw6quvYs+ePdi8eTOioqKkXqZPn47x48ejd+/e6Nu3L1auXIn8/Hy88sor1t8xREREZH+EHQNg8rF+/XohhBAZGRli0KBBonHjxkKpVIo2bdqImTNnCq1Wa7Cd9PR0MXz4cOHi4iKaNm0qZsyYIUpKSgxq9u7dK7p37y4UCoVo1aqV9B73+/jjj0XLli2FQqEQffv2FYcPH66uj14lWq1WADD63GR53NfWwf1sHdzP1sH9bB32sJ/t+kiVqOQsex8fH+zfv7/S7fj6+hr9vPdXgwcPxokTJyqsmTx5cpV+7rMWpVKJBQsWGJy/RdWD+9o6uJ+tg/vZOrifrcMe9nONWvyTiIiIyF7VqMU/iYiIiOwVQxURERGRBTBUEREREVkAQxURERGRBTBU1QJr1qyBn58fnJ2dERgYaLSCPBmKi4vDE088AW9vb8hkMmzbts1gXgiB+fPnw8vLCy4uLggODsa5c+cMarKzszFu3Di4urrC3d0dEyZMQF5enkHNyZMnMXDgQDg7O8PHxwfLli2r7o9mNyIiItCnTx80bNgQHh4eGDlyJFJTUw1qCgsLERYWhiZNmqBBgwYYNWqU0V0LMjIyEBoainr16sHDwwMzZ85EaWmpQc2+ffvQs2dPKJVKtGnTBhs2bKjuj2dX1q5di65du0oLHgYFBWHnzp3SPPez5UVGRkImk2Hq1KnSGPezZSxcuBAymczg0aFDB2ne7vezzRZzIIv4/vvvhUKhEF988YU4ffq0eP3114W7u7vIzMy0dWt2a8eOHWLu3Lnixx9/FADE1q1bDeYjIyOFm5ub2LZtm0hKShJPPvmk8Pf3FwUFBVJNSEiI6Natmzh8+LD47bffRJs2bcTYsWOlea1WKzw9PcW4ceNEcnKy+O6774SLi4v47LPPrPUxbUqtVov169eL5ORkkZiYKEaMGCFatmwp8vLypJqJEycKHx8fERsbK44dOyb69esn+vfvL82XlpaKLl26iODgYHHixAmxY8cO0bRpUzFnzhyp5uLFi6JevXpi+vTp4syZM+Ljjz8WDg4OIjo62qqf15Z+/vlnERUVJf744w+Rmpoq3n33XeHk5CSSk5OFENzPlnb06FHh5+cnunbtKt5++21pnPvZMhYsWCA6d+4srl+/Lj1u3Lghzdv7fmaoquH69u0rwsLCpOdlZWXC29tbRERE2LCrmuOvoUqv1wuVSiWWL18ujeXk5AilUim+++47IYQQZ86cEQDE77//LtXs3LlTyGQycfXqVSGEEJ9++qlo1KiRKCoqkmrCw8NF+/btq/kT2aesrCwBQOzfv18IcXefOjk5iS1btkg1Z8+eFQBEfHy8EOJu+JXL5UKj0Ug1a9euFa6urtJ+nTVrlujcubPBe40ePVqo1erq/kh2rVGjRuI///kP97OF5ebmirZt24qYmBjx6KOPSqGK+9lyFixYILp162ZyribsZ/78V4MVFxcjISEBwcHB0phcLkdwcDDi4+Nt2FnNlZaWBo1GY7BP3dzcEBgYKO3T+Ph4uLu7o3fv3lJNcHAw5HI5jhw5ItUMGjQICoVCqlGr1UhNTcXt27et9Gnsh1arBQA0btwYAJCQkICSkhKD/dyhQwe0bNnSYD8HBATA09NTqlGr1dDpdDh9+rRUc/827tXU1b//ZWVl+P7775Gfn4+goCDuZwsLCwtDaGio0b7gfrasc+fOwdvbG61atcK4ceOQkZEBoGbsZ4aqGuzmzZsoKysz+MsDAJ6entJ9Eqlq7u23ivapRqORbs59j6OjIxo3bmxQY2ob979HXaHX6zF16lQ88sgj6NKlC4C7+0ChUMDd3d2g9q/7ubJ9WF6NTqdDQUFBdXwcu3Tq1Ck0aNAASqUSEydOxNatW9GpUyfuZwv6/vvvcfz4cURERBjNcT9bTmBgIDZs2IDo6GisXbsWaWlpGDhwIHJzc2vEfrbr29QQUc0XFhaG5ORkHDhwwNat1Frt27dHYmIitFotfvjhB4wfP96sW3iReS5fvoy3334bMTExcHZ2tnU7tdrw4cOlP3ft2hWBgYHw9fXF5s2b4eLiYsPOzMMjVTVY06ZN4eDgYHTlQ2ZmJlQqlY26qtnu7beK9qlKpUJWVpbBfGlpKbKzsw1qTG3j/veoCyZPnozt27dj7969aNGihTSuUqlQXFyMnJwcg/q/7ufK9mF5Na6urjXiP8CWolAo0KZNG/Tq1QsRERHo1q0bVq1axf1sIQkJCcjKykLPnj3h6OgIR0dH7N+/H6tXr4ajoyM8PT25n6uJu7s72rVrh/Pnz9eIv88MVTWYQqFAr169EBsbK43p9XrExsYiKCjIhp3VXP7+/lCpVAb7VKfT4ciRI9I+DQoKQk5ODhISEqSaPXv2QK/XIzAwUKqJi4tDSUmJVBMTE4P27dujUaNGVvo0tiOEwOTJk7F161bs2bMH/v7+BvO9evWCk5OTwX5OTU1FRkaGwX4+deqUQYCNiYmBq6srOnXqJNXcv417NXX9779er0dRURH3s4UMHToUp06dQmJiovTo3bs3xo0bJ/2Z+7l65OXl4cKFC/Dy8qoZf58f+lR3sqnvv/9eKJVKsWHDBnHmzBnxxhtvCHd3d4MrH8hQbm6uOHHihDhx4oQAID788ENx4sQJcenSJSHE3SUV3N3dxU8//SROnjwpnnrqKZNLKvTo0UMcOXJEHDhwQLRt29ZgSYWcnBzh6ekpXnzxRZGcnCy+//57Ua9evTqzpMKkSZOEm5ub2Ldvn8Gl0Xfu3JFqJk6cKFq2bCn27Nkjjh07JoKCgkRQUJA0f+/S6GHDhonExEQRHR0tmjVrZvLS6JkzZ4qzZ8+KNWvW1LlL0GfPni32798v0tLSxMmTJ8Xs2bOFTCYTv/76qxCC+7m63H/1nxDcz5YyY8YMsW/fPpGWliYOHjwogoODRdOmTUVWVpYQwv73M0NVLfDxxx+Lli1bCoVCIfr27SsOHz5s65bs2t69ewUAo8f48eOFEHeXVZg3b57w9PQUSqVSDB06VKSmphps49atW2Ls2LGiQYMGwtXVVbzyyisiNzfXoCYpKUkMGDBAKJVK0bx5cxEZGWmtj2hzpvYvALF+/XqppqCgQLz11luiUaNGol69euLpp58W169fN9hOenq6GD58uHBxcRFNmzYVM2bMECUlJQY1e/fuFd27dxcKhUK0atXK4D3qgldffVX4+voKhUIhmjVrJoYOHSoFKiG4n6vLX0MV97NljB49Wnh5eQmFQiGaN28uRo8eLc6fPy/N2/t+lgkhxMMf7yIiIiKq23hOFREREZEFMFQRERERWQBDFREREZEFMFQRERERWQBDFREREZEFMFQRERERWQBDFREREZEFMFQRERERWQBDFREREZEFMFQRUa328ssvQyaTGT1CQkIAAH5+ftJY/fr10bNnT2zZssVgG9nZ2Zg6dSp8fX2hUCjg7e2NV199FRkZGUbvp9FoMGXKFLRq1QpKpRI+Pj544oknDG7g6ufnh5UrVxq9duHChejevbv0/M6dO5gzZw5at24NZ2dnNGvWDI8++ih++ukny+wcIrIoR1s3QERU3UJCQrB+/XqDMaVSKf158eLFeP3116HT6bBixQqMHj0azZs3R//+/ZGdnY1+/fpBoVBg3bp16Ny5M9LT0/Hee++hT58+iI+PR6tWrQAA6enpeOSRR+Du7o7ly5cjICAAJSUl2LVrF8LCwpCSklKlvidOnIgjR47g448/RqdOnXDr1i0cOnQIt27devidQkQWx1BFRLWeUqmESqUqd75hw4ZQqVRQqVRYs2YNvv76a/zyyy/o378/5s6di2vXruH8+fPSNlq2bIldu3ahbdu2CAsLw86dOwEAb731FmQyGY4ePYr69etL2+/cuTNeffXVKvf9888/Y9WqVRgxYgSAu0e4evXqVeXtEJF18Oc/IqL7ODo6wsnJCcXFxdDr9fj+++8xbtw4o1Dm4uKCt956C7t27UJ2djays7MRHR2NsLAwg0B1j7u7e5V7UalU2LFjB3Jzcx/04xCRFTFUEVGtt337djRo0MDgsXTpUqO64uJiREREQKvV4rHHHsONGzeQk5ODjh07mtxux44dIYTA+fPncf78eQgh0KFDB7N6Cg8Pr7Snzz//HIcOHUKTJk3Qp08fTJs2DQcPHqz6DiAiq+DPf0RU6w0ZMgRr1641GGvcuLH05/DwcLz33nsoLCxEgwYNEBkZidDQUGRmZgIAhBCVvoc5NfebOXMmXn75ZYOx1atXIy4uTno+aNAgXLx4EYcPH8ahQ4cQGxuLVatWYdGiRZg3b16V3o+Iqh9DFRHVevXr10ebNm3Knb8XcBo0aABPT0/IZDIAQLNmzeDu7o6zZ8+afN3Zs2chk8mkbctkMrNPRm/atKlRT/cHvXucnJwwcOBADBw4EOHh4ViyZAkWL16M8PBwKBQKs96LiKyDP/8RUZ13L+CoVCopUAGAXC7Hc889h2+//RYajcbgNQUFBfj000+hVqvRuHFjNG7cGGq1GmvWrEF+fr7Re+Tk5Fik106dOqG0tBSFhYUW2R4RWQ5DFRHVekVFRdBoNAaPmzdvmvXapUuXQqVS4W9/+xt27tyJy5cvIy4uDmq1GiUlJVizZo1Uu2bNGpSVlaFv3774v//7P5w7dw5nz57F6tWrERQUVOW+Bw8ejM8++wwJCQlIT0/Hjh078O6772LIkCFwdXWt8vaIqHrx5z8iqvWio6Ph5eVlMNa+fXuzfqpr0qQJDh8+jMWLF+PNN9+ERqNB48aNMXz4cHz99ddo2bKlVNuqVSscP34cH3zwAWbMmIHr16+jWbNm6NWrl9E5XeZQq9XYuHEj3n33Xdy5cwfe3t54/PHHMX/+/Cpvi4iqn0xU9exKIiIiIjLCn/+IiIiILIChioiIiMgCGKqIiIiILIChioiIiMgCGKqIiIiILIChioiIiMgCGKqIiIiILIChioiIiMgCGKqIiIiILIChioiIiMgCGKqIiIiILOD/ATK8M7BEK1IWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abe482e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 48541.8828125\n"
     ]
    }
   ],
   "source": [
    "# validate the test data \n",
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print(f\"RMSE: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3d863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=['Test'])\n",
    "data_predicted = pd.DataFrame(y_pred.tolist(),columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f96714fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>162262.515625</td>\n",
       "      <td>-32262.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>184557.781250</td>\n",
       "      <td>-45670.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>152551.687500</td>\n",
       "      <td>22948.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>267787.968750</td>\n",
       "      <td>-72787.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>187093.062500</td>\n",
       "      <td>-44593.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test     Prediction    Difference\n",
       "0  130000.0  162262.515625 -32262.515625\n",
       "1  138887.0  184557.781250 -45670.781250\n",
       "2  175500.0  152551.687500  22948.312500\n",
       "3  195000.0  267787.968750 -72787.968750\n",
       "4  142500.0  187093.062500 -44593.062500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9639d",
   "metadata": {},
   "source": [
    "## saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fdf75e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'HouseWeights.pt')\n",
    "model.load_state_dict(torch.load('HouseWeights.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "003d1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 31119.447265625\n"
     ]
    }
   ],
   "source": [
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print(f\"RMSE: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a671169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
