{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24f77da",
   "metadata": {},
   "source": [
    "# house price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb404d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bbb6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('datasets/houseprice.csv',usecols=['SalePrice','MSSubClass','MSZoning','LotFrontage','LotArea','Street','YearBuilt','LotShape','1stFlrSF','2ndFlrSF']).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727f63ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass     0\n",
       "MSZoning       0\n",
       "LotFrontage    0\n",
       "LotArea        0\n",
       "Street         0\n",
       "LotShape       0\n",
       "YearBuilt      0\n",
       "1stFlrSF       0\n",
       "2ndFlrSF       0\n",
       "SalePrice      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2c6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   object \n",
      " 5   LotShape     1201 non-null   object \n",
      " 6   YearBuilt    1201 non-null   int64  \n",
      " 7   1stFlrSF     1201 non-null   int64  \n",
      " 8   2ndFlrSF     1201 non-null   int64  \n",
      " 9   SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371dcaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name MSSubClass are unique values are 15\n",
      "column name MSZoning are unique values are 5\n",
      "column name LotFrontage are unique values are 110\n",
      "column name LotArea are unique values are 869\n",
      "column name Street are unique values are 2\n",
      "column name LotShape are unique values are 4\n",
      "column name YearBuilt are unique values are 112\n",
      "column name 1stFlrSF are unique values are 678\n",
      "column name 2ndFlrSF are unique values are 368\n",
      "column name SalePrice are unique values are 597\n"
     ]
    }
   ],
   "source": [
    "# choose categorical features \n",
    "for i in df.columns:\n",
    "    print(f'column name {i} are unique values are {len(df[i].unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5248976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4857fa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>today year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  1stFlrSF  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       856   \n",
       "1          20       RL         80.0     9600   Pave      Reg      1262   \n",
       "2          60       RL         68.0    11250   Pave      IR1       920   \n",
       "3          70       RL         60.0     9550   Pave      IR1       961   \n",
       "4          60       RL         84.0    14260   Pave      IR1      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  today year  \n",
       "0       854     208500          22  \n",
       "1         0     181500          49  \n",
       "2       866     223500          24  \n",
       "3       756     140000         110  \n",
       "4      1053     250000          25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['today year']=datetime.datetime.now().year-df['YearBuilt']\n",
    "df.drop('YearBuilt',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf98275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between categorical and contineous features\n",
    "cat_features=['MSSubClass','MSZoning','Street','LotShape']\n",
    "out_feature='SalePrice' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d93e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  20,  70,  50, 190,  45,  90, 120,  30,  80, 160,  75, 180,\n",
       "        40,  85], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MSSubClass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176e0c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders={}\n",
    "lbl_encoders['MSSubClass']=LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b8a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fucction for label encoding \n",
    "for feature in cat_features:\n",
    "    lbl_encoders[feature]=LabelEncoder()\n",
    "    df[feature]=lbl_encoders[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8352f5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>today year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0           5         3         65.0     8450       1         3       856   \n",
       "1           0         3         80.0     9600       1         3      1262   \n",
       "2           5         3         68.0    11250       1         0       920   \n",
       "3           6         3         60.0     9550       1         0       961   \n",
       "4           5         3         84.0    14260       1         0      1145   \n",
       "\n",
       "   2ndFlrSF  SalePrice  today year  \n",
       "0       854     208500          22  \n",
       "1         0     181500          49  \n",
       "2       866     223500          24  \n",
       "3       756     140000         110  \n",
       "4      1053     250000          25  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b528f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stackinh and coverting into tensors \n",
    "cat_features=np.stack([df['MSSubClass'],df['MSZoning'],df['Street'],df['LotShape']],1)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be8d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy to tensors \n",
    "import torch\n",
    "cat_features=torch.tensor(cat_features,dtype=torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b11b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'today year']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continous variables \n",
    "cont_features=[]\n",
    "for i in df.columns:\n",
    "    if i in ['MSSubClass','MSZoning','Street','LotShape','SalePrice']:\n",
    "        pass \n",
    "    else:\n",
    "        cont_features.append(i)\n",
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f5bc899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    22.],\n",
       "        [   80.,  9600.,  1262.,     0.,    49.],\n",
       "        [   68., 11250.,   920.,   866.,    24.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    84.],\n",
       "        [   68.,  9717.,  1078.,     0.,    75.],\n",
       "        [   75.,  9937.,  1256.,     0.,    60.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacking continuous varaibles to a tensor \n",
    "cont_values=np.stack([df[i].values for i in cont_features],axis=1)\n",
    "cont_values=torch.tensor(cont_values,dtype=torch.float)\n",
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff43ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependent features \n",
    "y=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffdc6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 4]), torch.Size([1201, 5]), torch.Size([1201, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.shape,cont_values.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37292b2",
   "metadata": {},
   "source": [
    "# embeddings size for categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cba4954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings layers to decide output based on input \n",
    "cat_dim=[len(df[col].unique()) for col in ['MSSubClass','MSZoning','Street','LotShape']]\n",
    "cat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0135abf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### thunb rule what dimension output shoud be set based on the input dimension \n",
    "embedding_dim=[(x,min(50,(x+1)//2)) for x in cat_dim]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0893db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layers to pytorch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef29fd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e09b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.7083, -0.4507,  0.8825,  ..., -0.8468,  1.0177, -0.4296],\n",
       "         [-1.2092, -1.3831, -0.0635,  ..., -0.5430, -1.3477, -0.8456],\n",
       "         [ 0.7083, -0.4507,  0.8825,  ..., -0.8468,  1.0177, -0.4296],\n",
       "         ...,\n",
       "         [-0.7929,  0.9918,  1.4041,  ..., -0.2628, -1.6985,  1.2523],\n",
       "         [-1.2092, -1.3831, -0.0635,  ..., -0.5430, -1.3477, -0.8456],\n",
       "         [-1.2092, -1.3831, -0.0635,  ..., -0.5430, -1.3477, -0.8456]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.3586, -1.1876, -0.3165],\n",
       "         [-0.3586, -1.1876, -0.3165],\n",
       "         [-0.3586, -1.1876, -0.3165],\n",
       "         ...,\n",
       "         [-0.3586, -1.1876, -0.3165],\n",
       "         [-0.3586, -1.1876, -0.3165],\n",
       "         [-0.3586, -1.1876, -0.3165]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[0.3990],\n",
       "         [0.3990],\n",
       "         [0.3990],\n",
       "         ...,\n",
       "         [0.3990],\n",
       "         [0.3990],\n",
       "         [0.3990]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.1301,  0.6004],\n",
       "         [-0.1301,  0.6004],\n",
       "         [ 0.2386,  1.8432],\n",
       "         ...,\n",
       "         [-0.1301,  0.6004],\n",
       "         [-0.1301,  0.6004],\n",
       "         [-0.1301,  0.6004]], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val=[]\n",
    "for i,e, in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_features[:,i]))\n",
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865f1e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7083, -0.4507,  0.8825,  ...,  0.3990, -0.1301,  0.6004],\n",
       "        [-1.2092, -1.3831, -0.0635,  ...,  0.3990, -0.1301,  0.6004],\n",
       "        [ 0.7083, -0.4507,  0.8825,  ...,  0.3990,  0.2386,  1.8432],\n",
       "        ...,\n",
       "        [-0.7929,  0.9918,  1.4041,  ...,  0.3990, -0.1301,  0.6004],\n",
       "        [-1.2092, -1.3831, -0.0635,  ...,  0.3990, -0.1301,  0.6004],\n",
       "        [-1.2092, -1.3831, -0.0635,  ...,  0.3990, -0.1301,  0.6004]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.cat(embedding_val,1)\n",
    "z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1c273f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.7512,  0.0000,  ...,  0.6649, -0.0000,  1.0007],\n",
       "        [-0.0000, -0.0000, -0.1058,  ...,  0.6649, -0.0000,  1.0007],\n",
       "        [ 1.1806, -0.7512,  1.4708,  ...,  0.6649,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-1.3216,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000, -2.3052, -0.0000,  ...,  0.6649, -0.0000,  1.0007],\n",
       "        [-2.0153, -2.3052, -0.1058,  ...,  0.0000, -0.2169,  1.0007]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement dropout \n",
    "dropout=nn.Dropout(0.4)\n",
    "final_embed=dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba3b1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a neural network \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_dim,n_cont,out_sz,layers,p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop=nn.Dropout(p)\n",
    "        self.bn_cont=nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layerlist=[]\n",
    "        n_emb=sum((out for inp,out in embedding_dim))\n",
    "        n_in=n_emb+n_cont \n",
    "\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in=i \n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        self.layers=nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self,x_cat,x_cont):\n",
    "        embeddings=[]\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x=torch.cat(embeddings,1)\n",
    "        x=self.emb_drop(x)\n",
    "        x_cont=self.bn_cont(x_cont)\n",
    "        x=torch.cat([x,x_cont],1)\n",
    "        x=self.layers(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97403977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fdef704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "model=FeedForwardNN(embedding_dim,len(cont_features),1,[100,50],p=0.4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b40337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5f900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "607a43e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   65.,  8450.,   856.,   854.,    22.],\n",
       "         [   80.,  9600.,  1262.,     0.,    49.],\n",
       "         [   68., 11250.,   920.,   866.,    24.],\n",
       "         ...,\n",
       "         [   66.,  9042.,  1188.,  1152.,    84.],\n",
       "         [   68.,  9717.,  1078.,     0.,    75.],\n",
       "         [   75.,  9937.,  1256.,     0.,    60.]]),\n",
       " torch.Size([1201, 5]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_values, cont_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "127092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "batch_size=1200\n",
    "test_size=int(batch_size*0.15)\n",
    "train_categorical=cat_features[:batch_size-test_size]\n",
    "test_categorical=cat_features[batch_size-test_size:batch_size]\n",
    "train_cont=cont_values[:batch_size-test_size]\n",
    "test_cont=cont_values[batch_size-test_size:batch_size]\n",
    "ytrain=y[:batch_size-test_size]\n",
    "y_test=y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9330908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(ytrain),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8b5be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 and loss 200496.375\n",
      "Epoch number: 11 and loss 200493.171875\n",
      "Epoch number: 21 and loss 200488.640625\n",
      "Epoch number: 31 and loss 200481.6875\n",
      "Epoch number: 41 and loss 200472.59375\n",
      "Epoch number: 51 and loss 200460.25\n",
      "Epoch number: 61 and loss 200445.0\n",
      "Epoch number: 71 and loss 200426.75\n",
      "Epoch number: 81 and loss 200406.453125\n",
      "Epoch number: 91 and loss 200379.34375\n",
      "Epoch number: 101 and loss 200351.796875\n",
      "Epoch number: 111 and loss 200319.15625\n",
      "Epoch number: 121 and loss 200286.609375\n",
      "Epoch number: 131 and loss 200244.5\n",
      "Epoch number: 141 and loss 200205.640625\n",
      "Epoch number: 151 and loss 200158.96875\n",
      "Epoch number: 161 and loss 200108.25\n",
      "Epoch number: 171 and loss 200058.359375\n",
      "Epoch number: 181 and loss 199998.5625\n",
      "Epoch number: 191 and loss 199938.578125\n",
      "Epoch number: 201 and loss 199871.71875\n",
      "Epoch number: 211 and loss 199806.125\n",
      "Epoch number: 221 and loss 199726.484375\n",
      "Epoch number: 231 and loss 199653.53125\n",
      "Epoch number: 241 and loss 199581.609375\n",
      "Epoch number: 251 and loss 199501.0625\n",
      "Epoch number: 261 and loss 199407.109375\n",
      "Epoch number: 271 and loss 199308.0\n",
      "Epoch number: 281 and loss 199216.8125\n",
      "Epoch number: 291 and loss 199123.96875\n",
      "Epoch number: 301 and loss 199027.75\n",
      "Epoch number: 311 and loss 198915.84375\n",
      "Epoch number: 321 and loss 198790.453125\n",
      "Epoch number: 331 and loss 198689.890625\n",
      "Epoch number: 341 and loss 198613.203125\n",
      "Epoch number: 351 and loss 198495.765625\n",
      "Epoch number: 361 and loss 198348.59375\n",
      "Epoch number: 371 and loss 198237.71875\n",
      "Epoch number: 381 and loss 198102.625\n",
      "Epoch number: 391 and loss 197968.265625\n",
      "Epoch number: 401 and loss 197847.078125\n",
      "Epoch number: 411 and loss 197701.71875\n",
      "Epoch number: 421 and loss 197585.0\n",
      "Epoch number: 431 and loss 197434.953125\n",
      "Epoch number: 441 and loss 197267.328125\n",
      "Epoch number: 451 and loss 197104.640625\n",
      "Epoch number: 461 and loss 196989.890625\n",
      "Epoch number: 471 and loss 196830.96875\n",
      "Epoch number: 481 and loss 196631.421875\n",
      "Epoch number: 491 and loss 196481.078125\n",
      "Epoch number: 501 and loss 196321.390625\n",
      "Epoch number: 511 and loss 196176.40625\n",
      "Epoch number: 521 and loss 195968.578125\n",
      "Epoch number: 531 and loss 195760.40625\n",
      "Epoch number: 541 and loss 195598.75\n",
      "Epoch number: 551 and loss 195448.359375\n",
      "Epoch number: 561 and loss 195306.671875\n",
      "Epoch number: 571 and loss 195061.203125\n",
      "Epoch number: 581 and loss 194835.71875\n",
      "Epoch number: 591 and loss 194629.5625\n",
      "Epoch number: 601 and loss 194518.546875\n",
      "Epoch number: 611 and loss 194289.6875\n",
      "Epoch number: 621 and loss 194104.53125\n",
      "Epoch number: 631 and loss 193897.53125\n",
      "Epoch number: 641 and loss 193748.65625\n",
      "Epoch number: 651 and loss 193363.9375\n",
      "Epoch number: 661 and loss 193266.90625\n",
      "Epoch number: 671 and loss 193051.96875\n",
      "Epoch number: 681 and loss 192824.765625\n",
      "Epoch number: 691 and loss 192664.203125\n",
      "Epoch number: 701 and loss 192338.609375\n",
      "Epoch number: 711 and loss 192243.59375\n",
      "Epoch number: 721 and loss 191933.9375\n",
      "Epoch number: 731 and loss 191794.703125\n",
      "Epoch number: 741 and loss 191381.859375\n",
      "Epoch number: 751 and loss 191283.921875\n",
      "Epoch number: 761 and loss 190975.390625\n",
      "Epoch number: 771 and loss 190656.140625\n",
      "Epoch number: 781 and loss 190390.953125\n",
      "Epoch number: 791 and loss 190211.015625\n",
      "Epoch number: 801 and loss 189864.71875\n",
      "Epoch number: 811 and loss 189718.828125\n",
      "Epoch number: 821 and loss 189520.5\n",
      "Epoch number: 831 and loss 189223.96875\n",
      "Epoch number: 841 and loss 188859.546875\n",
      "Epoch number: 851 and loss 188725.40625\n",
      "Epoch number: 861 and loss 188434.921875\n",
      "Epoch number: 871 and loss 188122.265625\n",
      "Epoch number: 881 and loss 187702.765625\n",
      "Epoch number: 891 and loss 187578.9375\n",
      "Epoch number: 901 and loss 187427.96875\n",
      "Epoch number: 911 and loss 187031.40625\n",
      "Epoch number: 921 and loss 186723.140625\n",
      "Epoch number: 931 and loss 186372.28125\n",
      "Epoch number: 941 and loss 186179.65625\n",
      "Epoch number: 951 and loss 185902.84375\n",
      "Epoch number: 961 and loss 185459.765625\n",
      "Epoch number: 971 and loss 185315.5\n",
      "Epoch number: 981 and loss 184870.078125\n",
      "Epoch number: 991 and loss 184524.59375\n",
      "Epoch number: 1001 and loss 184409.828125\n",
      "Epoch number: 1011 and loss 184240.171875\n",
      "Epoch number: 1021 and loss 183613.328125\n",
      "Epoch number: 1031 and loss 183346.515625\n",
      "Epoch number: 1041 and loss 183067.765625\n",
      "Epoch number: 1051 and loss 183003.859375\n",
      "Epoch number: 1061 and loss 182722.890625\n",
      "Epoch number: 1071 and loss 182447.734375\n",
      "Epoch number: 1081 and loss 181614.109375\n",
      "Epoch number: 1091 and loss 181671.671875\n",
      "Epoch number: 1101 and loss 181348.515625\n",
      "Epoch number: 1111 and loss 180744.4375\n",
      "Epoch number: 1121 and loss 180517.796875\n",
      "Epoch number: 1131 and loss 180146.0\n",
      "Epoch number: 1141 and loss 179781.71875\n",
      "Epoch number: 1151 and loss 179554.140625\n",
      "Epoch number: 1161 and loss 179319.65625\n",
      "Epoch number: 1171 and loss 179128.140625\n",
      "Epoch number: 1181 and loss 178539.84375\n",
      "Epoch number: 1191 and loss 178407.953125\n",
      "Epoch number: 1201 and loss 177493.921875\n",
      "Epoch number: 1211 and loss 177410.96875\n",
      "Epoch number: 1221 and loss 176970.453125\n",
      "Epoch number: 1231 and loss 176884.0625\n",
      "Epoch number: 1241 and loss 176093.046875\n",
      "Epoch number: 1251 and loss 176076.4375\n",
      "Epoch number: 1261 and loss 175545.78125\n",
      "Epoch number: 1271 and loss 175071.875\n",
      "Epoch number: 1281 and loss 174671.25\n",
      "Epoch number: 1291 and loss 174370.265625\n",
      "Epoch number: 1301 and loss 174341.1875\n",
      "Epoch number: 1311 and loss 173617.53125\n",
      "Epoch number: 1321 and loss 173489.640625\n",
      "Epoch number: 1331 and loss 172812.75\n",
      "Epoch number: 1341 and loss 172737.171875\n",
      "Epoch number: 1351 and loss 172564.609375\n",
      "Epoch number: 1361 and loss 172259.734375\n",
      "Epoch number: 1371 and loss 171585.734375\n",
      "Epoch number: 1381 and loss 171221.125\n",
      "Epoch number: 1391 and loss 171194.171875\n",
      "Epoch number: 1401 and loss 170061.21875\n",
      "Epoch number: 1411 and loss 169971.015625\n",
      "Epoch number: 1421 and loss 169711.9375\n",
      "Epoch number: 1431 and loss 169674.9375\n",
      "Epoch number: 1441 and loss 168930.796875\n",
      "Epoch number: 1451 and loss 168597.765625\n",
      "Epoch number: 1461 and loss 167847.9375\n",
      "Epoch number: 1471 and loss 167320.59375\n",
      "Epoch number: 1481 and loss 167351.21875\n",
      "Epoch number: 1491 and loss 166817.421875\n",
      "Epoch number: 1501 and loss 166251.34375\n",
      "Epoch number: 1511 and loss 166096.15625\n",
      "Epoch number: 1521 and loss 165455.3125\n",
      "Epoch number: 1531 and loss 165254.8125\n",
      "Epoch number: 1541 and loss 164822.109375\n",
      "Epoch number: 1551 and loss 164463.9375\n",
      "Epoch number: 1561 and loss 163745.5625\n",
      "Epoch number: 1571 and loss 163279.21875\n",
      "Epoch number: 1581 and loss 162953.484375\n",
      "Epoch number: 1591 and loss 162679.015625\n",
      "Epoch number: 1601 and loss 162021.390625\n",
      "Epoch number: 1611 and loss 161710.078125\n",
      "Epoch number: 1621 and loss 160945.59375\n",
      "Epoch number: 1631 and loss 160975.4375\n",
      "Epoch number: 1641 and loss 160648.3125\n",
      "Epoch number: 1651 and loss 160010.5625\n",
      "Epoch number: 1661 and loss 159091.546875\n",
      "Epoch number: 1671 and loss 159700.734375\n",
      "Epoch number: 1681 and loss 158841.9375\n",
      "Epoch number: 1691 and loss 158355.453125\n",
      "Epoch number: 1701 and loss 157889.765625\n",
      "Epoch number: 1711 and loss 157569.640625\n",
      "Epoch number: 1721 and loss 157389.875\n",
      "Epoch number: 1731 and loss 156466.28125\n",
      "Epoch number: 1741 and loss 156358.15625\n",
      "Epoch number: 1751 and loss 155969.734375\n",
      "Epoch number: 1761 and loss 154807.25\n",
      "Epoch number: 1771 and loss 154757.9375\n",
      "Epoch number: 1781 and loss 154028.34375\n",
      "Epoch number: 1791 and loss 152982.640625\n",
      "Epoch number: 1801 and loss 153602.46875\n",
      "Epoch number: 1811 and loss 153134.1875\n",
      "Epoch number: 1821 and loss 152296.859375\n",
      "Epoch number: 1831 and loss 152029.015625\n",
      "Epoch number: 1841 and loss 151239.25\n",
      "Epoch number: 1851 and loss 151347.375\n",
      "Epoch number: 1861 and loss 150566.484375\n",
      "Epoch number: 1871 and loss 149840.734375\n",
      "Epoch number: 1881 and loss 150150.234375\n",
      "Epoch number: 1891 and loss 149627.0625\n",
      "Epoch number: 1901 and loss 149020.890625\n",
      "Epoch number: 1911 and loss 147683.015625\n",
      "Epoch number: 1921 and loss 148196.640625\n",
      "Epoch number: 1931 and loss 147399.046875\n",
      "Epoch number: 1941 and loss 147179.296875\n",
      "Epoch number: 1951 and loss 146509.421875\n",
      "Epoch number: 1961 and loss 146743.6875\n",
      "Epoch number: 1971 and loss 145374.453125\n",
      "Epoch number: 1981 and loss 144650.265625\n",
      "Epoch number: 1991 and loss 144026.53125\n",
      "Epoch number: 2001 and loss 144178.09375\n",
      "Epoch number: 2011 and loss 143841.765625\n",
      "Epoch number: 2021 and loss 143194.3125\n",
      "Epoch number: 2031 and loss 143110.484375\n",
      "Epoch number: 2041 and loss 142002.0\n",
      "Epoch number: 2051 and loss 142239.703125\n",
      "Epoch number: 2061 and loss 141132.421875\n",
      "Epoch number: 2071 and loss 140542.796875\n",
      "Epoch number: 2081 and loss 140584.453125\n",
      "Epoch number: 2091 and loss 139317.875\n",
      "Epoch number: 2101 and loss 139110.46875\n",
      "Epoch number: 2111 and loss 138195.0625\n",
      "Epoch number: 2121 and loss 138312.890625\n",
      "Epoch number: 2131 and loss 138137.09375\n",
      "Epoch number: 2141 and loss 137266.828125\n",
      "Epoch number: 2151 and loss 137272.625\n",
      "Epoch number: 2161 and loss 136791.8125\n",
      "Epoch number: 2171 and loss 135552.40625\n",
      "Epoch number: 2181 and loss 135598.15625\n",
      "Epoch number: 2191 and loss 134400.3125\n",
      "Epoch number: 2201 and loss 133962.734375\n",
      "Epoch number: 2211 and loss 134149.625\n",
      "Epoch number: 2221 and loss 133516.828125\n",
      "Epoch number: 2231 and loss 132460.546875\n",
      "Epoch number: 2241 and loss 132715.578125\n",
      "Epoch number: 2251 and loss 132129.265625\n",
      "Epoch number: 2261 and loss 131729.59375\n",
      "Epoch number: 2271 and loss 131097.921875\n",
      "Epoch number: 2281 and loss 130018.0\n",
      "Epoch number: 2291 and loss 129855.6875\n",
      "Epoch number: 2301 and loss 128505.78125\n",
      "Epoch number: 2311 and loss 128874.6484375\n",
      "Epoch number: 2321 and loss 127604.1171875\n",
      "Epoch number: 2331 and loss 127645.9609375\n",
      "Epoch number: 2341 and loss 127310.625\n",
      "Epoch number: 2351 and loss 127668.78125\n",
      "Epoch number: 2361 and loss 126011.7421875\n",
      "Epoch number: 2371 and loss 125159.671875\n",
      "Epoch number: 2381 and loss 125235.5390625\n",
      "Epoch number: 2391 and loss 125049.0390625\n",
      "Epoch number: 2401 and loss 124096.046875\n",
      "Epoch number: 2411 and loss 123248.0546875\n",
      "Epoch number: 2421 and loss 123529.34375\n",
      "Epoch number: 2431 and loss 122619.8515625\n",
      "Epoch number: 2441 and loss 123285.09375\n",
      "Epoch number: 2451 and loss 121546.96875\n",
      "Epoch number: 2461 and loss 121145.3515625\n",
      "Epoch number: 2471 and loss 120195.5078125\n",
      "Epoch number: 2481 and loss 120886.7265625\n",
      "Epoch number: 2491 and loss 119757.1484375\n",
      "Epoch number: 2501 and loss 120026.328125\n",
      "Epoch number: 2511 and loss 118617.2265625\n",
      "Epoch number: 2521 and loss 118135.2421875\n",
      "Epoch number: 2531 and loss 117302.125\n",
      "Epoch number: 2541 and loss 117640.0390625\n",
      "Epoch number: 2551 and loss 116855.4375\n",
      "Epoch number: 2561 and loss 115705.3125\n",
      "Epoch number: 2571 and loss 115341.6171875\n",
      "Epoch number: 2581 and loss 114132.09375\n",
      "Epoch number: 2591 and loss 115336.578125\n",
      "Epoch number: 2601 and loss 114151.296875\n",
      "Epoch number: 2611 and loss 114396.875\n",
      "Epoch number: 2621 and loss 113883.96875\n",
      "Epoch number: 2631 and loss 113472.78125\n",
      "Epoch number: 2641 and loss 112370.71875\n",
      "Epoch number: 2651 and loss 112972.3984375\n",
      "Epoch number: 2661 and loss 111000.1875\n",
      "Epoch number: 2671 and loss 110491.0703125\n",
      "Epoch number: 2681 and loss 109678.3671875\n",
      "Epoch number: 2691 and loss 109905.7578125\n",
      "Epoch number: 2701 and loss 109918.515625\n",
      "Epoch number: 2711 and loss 108027.359375\n",
      "Epoch number: 2721 and loss 108207.1171875\n",
      "Epoch number: 2731 and loss 108273.671875\n",
      "Epoch number: 2741 and loss 106165.4375\n",
      "Epoch number: 2751 and loss 108116.1171875\n",
      "Epoch number: 2761 and loss 105795.3671875\n",
      "Epoch number: 2771 and loss 105650.3828125\n",
      "Epoch number: 2781 and loss 105050.546875\n",
      "Epoch number: 2791 and loss 104884.7890625\n",
      "Epoch number: 2801 and loss 104465.7265625\n",
      "Epoch number: 2811 and loss 103107.0\n",
      "Epoch number: 2821 and loss 103240.578125\n",
      "Epoch number: 2831 and loss 102826.8984375\n",
      "Epoch number: 2841 and loss 102344.8515625\n",
      "Epoch number: 2851 and loss 101357.890625\n",
      "Epoch number: 2861 and loss 101824.4453125\n",
      "Epoch number: 2871 and loss 100394.078125\n",
      "Epoch number: 2881 and loss 100608.5625\n",
      "Epoch number: 2891 and loss 100610.5625\n",
      "Epoch number: 2901 and loss 99573.125\n",
      "Epoch number: 2911 and loss 99258.7421875\n",
      "Epoch number: 2921 and loss 97475.359375\n",
      "Epoch number: 2931 and loss 97387.8125\n",
      "Epoch number: 2941 and loss 97357.4296875\n",
      "Epoch number: 2951 and loss 96573.09375\n",
      "Epoch number: 2961 and loss 95933.6328125\n",
      "Epoch number: 2971 and loss 95358.7578125\n",
      "Epoch number: 2981 and loss 95263.390625\n",
      "Epoch number: 2991 and loss 95408.8984375\n",
      "Epoch number: 3001 and loss 93154.734375\n",
      "Epoch number: 3011 and loss 93725.8984375\n",
      "Epoch number: 3021 and loss 92852.6171875\n",
      "Epoch number: 3031 and loss 93736.9609375\n",
      "Epoch number: 3041 and loss 92042.109375\n",
      "Epoch number: 3051 and loss 91332.515625\n",
      "Epoch number: 3061 and loss 91581.6796875\n",
      "Epoch number: 3071 and loss 90926.453125\n",
      "Epoch number: 3081 and loss 91665.5078125\n",
      "Epoch number: 3091 and loss 90462.125\n",
      "Epoch number: 3101 and loss 90037.71875\n",
      "Epoch number: 3111 and loss 88812.78125\n",
      "Epoch number: 3121 and loss 88813.5078125\n",
      "Epoch number: 3131 and loss 88589.65625\n",
      "Epoch number: 3141 and loss 87520.609375\n",
      "Epoch number: 3151 and loss 86112.3359375\n",
      "Epoch number: 3161 and loss 85810.4296875\n",
      "Epoch number: 3171 and loss 85260.703125\n",
      "Epoch number: 3181 and loss 85602.8203125\n",
      "Epoch number: 3191 and loss 85862.3359375\n",
      "Epoch number: 3201 and loss 85224.71875\n",
      "Epoch number: 3211 and loss 83800.4140625\n",
      "Epoch number: 3221 and loss 84328.53125\n",
      "Epoch number: 3231 and loss 83569.171875\n",
      "Epoch number: 3241 and loss 82811.4765625\n",
      "Epoch number: 3251 and loss 82877.1875\n",
      "Epoch number: 3261 and loss 81449.578125\n",
      "Epoch number: 3271 and loss 80630.9609375\n",
      "Epoch number: 3281 and loss 79979.5859375\n",
      "Epoch number: 3291 and loss 81105.9921875\n",
      "Epoch number: 3301 and loss 80394.6640625\n",
      "Epoch number: 3311 and loss 79042.8984375\n",
      "Epoch number: 3321 and loss 78231.703125\n",
      "Epoch number: 3331 and loss 78144.140625\n",
      "Epoch number: 3341 and loss 77565.0625\n",
      "Epoch number: 3351 and loss 77545.1875\n",
      "Epoch number: 3361 and loss 77139.3515625\n",
      "Epoch number: 3371 and loss 75232.109375\n",
      "Epoch number: 3381 and loss 76181.9140625\n",
      "Epoch number: 3391 and loss 74886.4765625\n",
      "Epoch number: 3401 and loss 73951.3359375\n",
      "Epoch number: 3411 and loss 75195.9140625\n",
      "Epoch number: 3421 and loss 73893.6875\n",
      "Epoch number: 3431 and loss 73316.03125\n",
      "Epoch number: 3441 and loss 71569.140625\n",
      "Epoch number: 3451 and loss 72255.578125\n",
      "Epoch number: 3461 and loss 72366.6328125\n",
      "Epoch number: 3471 and loss 71386.25\n",
      "Epoch number: 3481 and loss 70282.6015625\n",
      "Epoch number: 3491 and loss 70471.09375\n",
      "Epoch number: 3501 and loss 70263.359375\n",
      "Epoch number: 3511 and loss 69553.703125\n",
      "Epoch number: 3521 and loss 68880.203125\n",
      "Epoch number: 3531 and loss 68113.5390625\n",
      "Epoch number: 3541 and loss 68148.2734375\n",
      "Epoch number: 3551 and loss 66278.2734375\n",
      "Epoch number: 3561 and loss 67176.6875\n",
      "Epoch number: 3571 and loss 66651.265625\n",
      "Epoch number: 3581 and loss 66125.5546875\n",
      "Epoch number: 3591 and loss 65430.69140625\n",
      "Epoch number: 3601 and loss 65856.5390625\n",
      "Epoch number: 3611 and loss 63014.4609375\n",
      "Epoch number: 3621 and loss 64846.1015625\n",
      "Epoch number: 3631 and loss 63277.4296875\n",
      "Epoch number: 3641 and loss 62697.09375\n",
      "Epoch number: 3651 and loss 63110.54296875\n",
      "Epoch number: 3661 and loss 61771.65234375\n",
      "Epoch number: 3671 and loss 61508.796875\n",
      "Epoch number: 3681 and loss 61661.90625\n",
      "Epoch number: 3691 and loss 60779.73828125\n",
      "Epoch number: 3701 and loss 61336.90625\n",
      "Epoch number: 3711 and loss 61216.6875\n",
      "Epoch number: 3721 and loss 59300.93359375\n",
      "Epoch number: 3731 and loss 58714.6484375\n",
      "Epoch number: 3741 and loss 59197.88671875\n",
      "Epoch number: 3751 and loss 58188.37890625\n",
      "Epoch number: 3761 and loss 58624.3359375\n",
      "Epoch number: 3771 and loss 58129.078125\n",
      "Epoch number: 3781 and loss 57498.36328125\n",
      "Epoch number: 3791 and loss 57155.046875\n",
      "Epoch number: 3801 and loss 56307.3359375\n",
      "Epoch number: 3811 and loss 55158.1328125\n",
      "Epoch number: 3821 and loss 54081.703125\n",
      "Epoch number: 3831 and loss 55459.12890625\n",
      "Epoch number: 3841 and loss 54172.01953125\n",
      "Epoch number: 3851 and loss 53530.9140625\n",
      "Epoch number: 3861 and loss 54145.23046875\n",
      "Epoch number: 3871 and loss 53036.54296875\n",
      "Epoch number: 3881 and loss 52821.609375\n",
      "Epoch number: 3891 and loss 53353.45703125\n",
      "Epoch number: 3901 and loss 51045.55859375\n",
      "Epoch number: 3911 and loss 52705.2109375\n",
      "Epoch number: 3921 and loss 50827.66015625\n",
      "Epoch number: 3931 and loss 51338.44140625\n",
      "Epoch number: 3941 and loss 50991.22265625\n",
      "Epoch number: 3951 and loss 50467.0703125\n",
      "Epoch number: 3961 and loss 50692.8203125\n",
      "Epoch number: 3971 and loss 49950.9765625\n",
      "Epoch number: 3981 and loss 48548.4765625\n",
      "Epoch number: 3991 and loss 49067.890625\n",
      "Epoch number: 4001 and loss 46770.91015625\n",
      "Epoch number: 4011 and loss 47793.30078125\n",
      "Epoch number: 4021 and loss 48696.328125\n",
      "Epoch number: 4031 and loss 48100.99609375\n",
      "Epoch number: 4041 and loss 46794.75\n",
      "Epoch number: 4051 and loss 45826.8203125\n",
      "Epoch number: 4061 and loss 45023.359375\n",
      "Epoch number: 4071 and loss 46721.53515625\n",
      "Epoch number: 4081 and loss 44827.35546875\n",
      "Epoch number: 4091 and loss 46587.046875\n",
      "Epoch number: 4101 and loss 45574.4921875\n",
      "Epoch number: 4111 and loss 45094.05078125\n",
      "Epoch number: 4121 and loss 44779.59375\n",
      "Epoch number: 4131 and loss 43783.68359375\n",
      "Epoch number: 4141 and loss 43615.9453125\n",
      "Epoch number: 4151 and loss 43624.51953125\n",
      "Epoch number: 4161 and loss 43336.828125\n",
      "Epoch number: 4171 and loss 42071.10546875\n",
      "Epoch number: 4181 and loss 42352.1796875\n",
      "Epoch number: 4191 and loss 44355.984375\n",
      "Epoch number: 4201 and loss 43644.7578125\n",
      "Epoch number: 4211 and loss 40869.31640625\n",
      "Epoch number: 4221 and loss 41183.15234375\n",
      "Epoch number: 4231 and loss 42969.01953125\n",
      "Epoch number: 4241 and loss 41572.44921875\n",
      "Epoch number: 4251 and loss 40479.5703125\n",
      "Epoch number: 4261 and loss 41052.85546875\n",
      "Epoch number: 4271 and loss 40451.3203125\n",
      "Epoch number: 4281 and loss 39203.91015625\n",
      "Epoch number: 4291 and loss 39426.73046875\n",
      "Epoch number: 4301 and loss 39573.59375\n",
      "Epoch number: 4311 and loss 39147.08984375\n",
      "Epoch number: 4321 and loss 40517.9453125\n",
      "Epoch number: 4331 and loss 39826.05078125\n",
      "Epoch number: 4341 and loss 39296.05078125\n",
      "Epoch number: 4351 and loss 37941.296875\n",
      "Epoch number: 4361 and loss 37501.3515625\n",
      "Epoch number: 4371 and loss 37897.8671875\n",
      "Epoch number: 4381 and loss 38041.2734375\n",
      "Epoch number: 4391 and loss 38659.578125\n",
      "Epoch number: 4401 and loss 36856.05859375\n",
      "Epoch number: 4411 and loss 39554.2109375\n",
      "Epoch number: 4421 and loss 37489.4765625\n",
      "Epoch number: 4431 and loss 37784.4140625\n",
      "Epoch number: 4441 and loss 36847.015625\n",
      "Epoch number: 4451 and loss 37849.734375\n",
      "Epoch number: 4461 and loss 36669.61328125\n",
      "Epoch number: 4471 and loss 36874.86328125\n",
      "Epoch number: 4481 and loss 36123.046875\n",
      "Epoch number: 4491 and loss 37594.6328125\n",
      "Epoch number: 4501 and loss 37186.30078125\n",
      "Epoch number: 4511 and loss 35156.8671875\n",
      "Epoch number: 4521 and loss 37544.875\n",
      "Epoch number: 4531 and loss 38593.484375\n",
      "Epoch number: 4541 and loss 35012.2421875\n",
      "Epoch number: 4551 and loss 36631.1015625\n",
      "Epoch number: 4561 and loss 36250.17578125\n",
      "Epoch number: 4571 and loss 34486.25\n",
      "Epoch number: 4581 and loss 36755.94140625\n",
      "Epoch number: 4591 and loss 35085.74609375\n",
      "Epoch number: 4601 and loss 36559.01953125\n",
      "Epoch number: 4611 and loss 37575.96875\n",
      "Epoch number: 4621 and loss 37489.234375\n",
      "Epoch number: 4631 and loss 36819.55078125\n",
      "Epoch number: 4641 and loss 35757.6796875\n",
      "Epoch number: 4651 and loss 37148.87109375\n",
      "Epoch number: 4661 and loss 37702.65625\n",
      "Epoch number: 4671 and loss 35455.47265625\n",
      "Epoch number: 4681 and loss 35105.75390625\n",
      "Epoch number: 4691 and loss 35437.8984375\n",
      "Epoch number: 4701 and loss 36470.7265625\n",
      "Epoch number: 4711 and loss 35981.5859375\n",
      "Epoch number: 4721 and loss 36447.89453125\n",
      "Epoch number: 4731 and loss 34893.94921875\n",
      "Epoch number: 4741 and loss 36963.92578125\n",
      "Epoch number: 4751 and loss 35037.6328125\n",
      "Epoch number: 4761 and loss 36496.2890625\n",
      "Epoch number: 4771 and loss 35260.7109375\n",
      "Epoch number: 4781 and loss 35103.26953125\n",
      "Epoch number: 4791 and loss 35008.7734375\n",
      "Epoch number: 4801 and loss 36280.29296875\n",
      "Epoch number: 4811 and loss 37131.0859375\n",
      "Epoch number: 4821 and loss 36809.91015625\n",
      "Epoch number: 4831 and loss 36313.90625\n",
      "Epoch number: 4841 and loss 33557.96484375\n",
      "Epoch number: 4851 and loss 35407.21484375\n",
      "Epoch number: 4861 and loss 33510.375\n",
      "Epoch number: 4871 and loss 37141.55859375\n",
      "Epoch number: 4881 and loss 35109.3125\n",
      "Epoch number: 4891 and loss 33362.921875\n",
      "Epoch number: 4901 and loss 35244.7265625\n",
      "Epoch number: 4911 and loss 36462.8984375\n",
      "Epoch number: 4921 and loss 34886.19921875\n",
      "Epoch number: 4931 and loss 35379.53515625\n",
      "Epoch number: 4941 and loss 33162.57421875\n",
      "Epoch number: 4951 and loss 34588.55859375\n",
      "Epoch number: 4961 and loss 34963.24609375\n",
      "Epoch number: 4971 and loss 35595.96484375\n",
      "Epoch number: 4981 and loss 33360.05859375\n",
      "Epoch number: 4991 and loss 34114.30078125\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "epochs=5000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    y_pred=model(train_categorical,train_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,ytrain)) ## RMSE\n",
    "    final_losses.append(loss.item())\n",
    "    if i%10==1:\n",
    "        print(f\"Epoch number: {i} and loss {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d400d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('RMSE loss')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abe482e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 46648.1328125\n"
     ]
    }
   ],
   "source": [
    "# validate the test data \n",
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print(f\"RMSE: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e3d863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=['Test'])\n",
    "data_predicted = pd.DataFrame(y_pred.tolist(),columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f96714fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>158207.437500</td>\n",
       "      <td>-28207.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>147796.750000</td>\n",
       "      <td>-8909.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>149071.437500</td>\n",
       "      <td>26428.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>239346.234375</td>\n",
       "      <td>-44346.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>140027.312500</td>\n",
       "      <td>2472.687500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test     Prediction    Difference\n",
       "0  130000.0  158207.437500 -28207.437500\n",
       "1  138887.0  147796.750000  -8909.750000\n",
       "2  175500.0  149071.437500  26428.562500\n",
       "3  195000.0  239346.234375 -44346.234375\n",
       "4  142500.0  140027.312500   2472.687500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9639d",
   "metadata": {},
   "source": [
    "## saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fdf75e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'HouseWeights.pt')\n",
    "model.load_state_dict(torch.load('HouseWeights.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "003d1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 29434.916015625\n"
     ]
    }
   ],
   "source": [
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print(f\"RMSE: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a671169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
